{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### **Deadline: 2018.11.04 23:59:59**\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.5\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.5765\t Accuracy 0.0700\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.5015\t Accuracy 0.1516\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2749\t Accuracy 0.2320\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.1959\t Accuracy 0.3131\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1552\t Accuracy 0.3809\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1303\t Accuracy 0.4394\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.1134\t Accuracy 0.4853\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.1012\t Accuracy 0.5211\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0919\t Accuracy 0.5510\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0845\t Accuracy 0.5751\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0786\t Accuracy 0.5957\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0737\t Average training accuracy 0.6139\n",
      "Epoch [0]\t Average validation loss 0.0221\t Average validation accuracy 0.8346\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0215\t Accuracy 0.8400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0230\t Accuracy 0.8061\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0228\t Accuracy 0.8142\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0229\t Accuracy 0.8112\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0226\t Accuracy 0.8149\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0224\t Accuracy 0.8182\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0222\t Accuracy 0.8218\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0221\t Accuracy 0.8236\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0219\t Accuracy 0.8263\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0218\t Accuracy 0.8282\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0217\t Accuracy 0.8300\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0215\t Average training accuracy 0.8318\n",
      "Epoch [1]\t Average validation loss 0.0181\t Average validation accuracy 0.8870\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0177\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0193\t Accuracy 0.8571\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0193\t Accuracy 0.8567\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0195\t Accuracy 0.8509\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0195\t Accuracy 0.8531\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0194\t Accuracy 0.8546\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0193\t Accuracy 0.8556\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0194\t Accuracy 0.8553\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0193\t Accuracy 0.8567\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0193\t Accuracy 0.8573\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0193\t Accuracy 0.8574\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0192\t Average training accuracy 0.8578\n",
      "Epoch [2]\t Average validation loss 0.0168\t Average validation accuracy 0.9004\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0169\t Accuracy 0.9100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0180\t Accuracy 0.8716\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0180\t Accuracy 0.8684\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0184\t Accuracy 0.8620\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0183\t Accuracy 0.8632\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0183\t Accuracy 0.8641\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0183\t Accuracy 0.8644\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0183\t Accuracy 0.8636\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0183\t Accuracy 0.8645\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0183\t Accuracy 0.8645\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0183\t Accuracy 0.8642\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0183\t Average training accuracy 0.8646\n",
      "Epoch [3]\t Average validation loss 0.0163\t Average validation accuracy 0.9040\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0166\t Accuracy 0.9100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0174\t Accuracy 0.8765\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0174\t Accuracy 0.8717\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0177\t Accuracy 0.8664\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0177\t Accuracy 0.8673\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0177\t Accuracy 0.8678\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0177\t Accuracy 0.8686\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0178\t Accuracy 0.8676\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0178\t Accuracy 0.8685\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0178\t Accuracy 0.8685\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0178\t Accuracy 0.8681\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0177\t Average training accuracy 0.8685\n",
      "Epoch [4]\t Average validation loss 0.0160\t Average validation accuracy 0.9068\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0165\t Accuracy 0.9100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0169\t Accuracy 0.8782\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0170\t Accuracy 0.8751\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0173\t Accuracy 0.8695\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0173\t Accuracy 0.8702\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0173\t Accuracy 0.8704\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0173\t Accuracy 0.8715\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0174\t Accuracy 0.8703\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0173\t Accuracy 0.8712\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0174\t Accuracy 0.8708\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0174\t Accuracy 0.8705\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0173\t Average training accuracy 0.8707\n",
      "Epoch [5]\t Average validation loss 0.0159\t Average validation accuracy 0.9084\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0164\t Accuracy 0.9200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0166\t Accuracy 0.8802\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0167\t Accuracy 0.8765\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0170\t Accuracy 0.8712\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0170\t Accuracy 0.8719\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0170\t Accuracy 0.8718\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0170\t Accuracy 0.8729\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0171\t Accuracy 0.8715\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0171\t Accuracy 0.8721\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0171\t Accuracy 0.8719\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0171\t Accuracy 0.8717\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0171\t Average training accuracy 0.8718\n",
      "Epoch [6]\t Average validation loss 0.0157\t Average validation accuracy 0.9100\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0162\t Accuracy 0.9300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0165\t Accuracy 0.8800\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0165\t Accuracy 0.8766\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0168\t Accuracy 0.8702\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0168\t Accuracy 0.8717\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0168\t Accuracy 0.8716\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0168\t Accuracy 0.8730\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0169\t Accuracy 0.8714\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0169\t Accuracy 0.8720\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0169\t Accuracy 0.8717\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0169\t Accuracy 0.8713\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0169\t Average training accuracy 0.8715\n",
      "Epoch [7]\t Average validation loss 0.0154\t Average validation accuracy 0.9122\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0159\t Accuracy 0.9300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0164\t Accuracy 0.8788\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0164\t Accuracy 0.8755\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0167\t Accuracy 0.8698\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0167\t Accuracy 0.8718\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0167\t Accuracy 0.8716\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0166\t Accuracy 0.8730\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0167\t Accuracy 0.8711\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0167\t Accuracy 0.8717\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0168\t Accuracy 0.8714\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0168\t Accuracy 0.8711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0167\t Average training accuracy 0.8713\n",
      "Epoch [8]\t Average validation loss 0.0150\t Average validation accuracy 0.9136\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0155\t Accuracy 0.9300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0162\t Accuracy 0.8784\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0162\t Accuracy 0.8758\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0165\t Accuracy 0.8707\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0165\t Accuracy 0.8724\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0164\t Accuracy 0.8722\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0164\t Accuracy 0.8736\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0165\t Accuracy 0.8718\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0165\t Accuracy 0.8724\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0165\t Accuracy 0.8721\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0165\t Accuracy 0.8717\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0165\t Average training accuracy 0.8720\n",
      "Epoch [9]\t Average validation loss 0.0147\t Average validation accuracy 0.9144\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0151\t Accuracy 0.9400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0160\t Accuracy 0.8818\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0160\t Accuracy 0.8775\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0162\t Accuracy 0.8721\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0163\t Accuracy 0.8738\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0162\t Accuracy 0.8736\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0161\t Accuracy 0.8750\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0163\t Accuracy 0.8731\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0163\t Accuracy 0.8736\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0163\t Accuracy 0.8732\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0163\t Accuracy 0.8729\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0163\t Average training accuracy 0.8731\n",
      "Epoch [10]\t Average validation loss 0.0144\t Average validation accuracy 0.9152\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0148\t Accuracy 0.9400\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0157\t Accuracy 0.8835\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0158\t Accuracy 0.8785\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0160\t Accuracy 0.8732\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0160\t Accuracy 0.8750\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0160\t Accuracy 0.8750\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0159\t Accuracy 0.8763\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0161\t Accuracy 0.8745\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0160\t Accuracy 0.8749\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0161\t Accuracy 0.8744\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0161\t Accuracy 0.8742\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0161\t Average training accuracy 0.8746\n",
      "Epoch [11]\t Average validation loss 0.0141\t Average validation accuracy 0.9160\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0145\t Accuracy 0.9400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0155\t Accuracy 0.8859\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0156\t Accuracy 0.8804\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0158\t Accuracy 0.8746\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0158\t Accuracy 0.8764\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0158\t Accuracy 0.8763\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0157\t Accuracy 0.8775\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0159\t Accuracy 0.8757\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0158\t Accuracy 0.8760\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0159\t Accuracy 0.8756\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0159\t Accuracy 0.8753\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0159\t Average training accuracy 0.8756\n",
      "Epoch [12]\t Average validation loss 0.0139\t Average validation accuracy 0.9166\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0143\t Accuracy 0.9400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0153\t Accuracy 0.8878\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0154\t Accuracy 0.8820\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0157\t Accuracy 0.8758\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0157\t Accuracy 0.8773\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0156\t Accuracy 0.8771\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0155\t Accuracy 0.8784\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0157\t Accuracy 0.8766\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0157\t Accuracy 0.8769\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0157\t Accuracy 0.8765\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0157\t Accuracy 0.8762\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0157\t Average training accuracy 0.8766\n",
      "Epoch [13]\t Average validation loss 0.0137\t Average validation accuracy 0.9182\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0141\t Accuracy 0.9400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0151\t Accuracy 0.8880\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0153\t Accuracy 0.8821\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0155\t Accuracy 0.8764\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0155\t Accuracy 0.8776\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0155\t Accuracy 0.8776\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0154\t Accuracy 0.8788\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0156\t Accuracy 0.8771\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0156\t Accuracy 0.8774\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0156\t Accuracy 0.8771\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0156\t Accuracy 0.8769\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0156\t Average training accuracy 0.8773\n",
      "Epoch [14]\t Average validation loss 0.0136\t Average validation accuracy 0.9180\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0139\t Accuracy 0.9300\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0150\t Accuracy 0.8894\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0151\t Accuracy 0.8829\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0154\t Accuracy 0.8772\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0154\t Accuracy 0.8784\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0154\t Accuracy 0.8784\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0153\t Accuracy 0.8797\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0155\t Accuracy 0.8779\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0155\t Accuracy 0.8783\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0155\t Accuracy 0.8779\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0155\t Accuracy 0.8777\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0155\t Average training accuracy 0.8780\n",
      "Epoch [15]\t Average validation loss 0.0135\t Average validation accuracy 0.9182\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0138\t Accuracy 0.9300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0149\t Accuracy 0.8902\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0151\t Accuracy 0.8840\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0153\t Accuracy 0.8781\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0153\t Accuracy 0.8791\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0153\t Accuracy 0.8792\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0152\t Accuracy 0.8805\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0154\t Accuracy 0.8786\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0154\t Accuracy 0.8790\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0154\t Accuracy 0.8786\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0154\t Accuracy 0.8782\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0154\t Average training accuracy 0.8784\n",
      "Epoch [16]\t Average validation loss 0.0134\t Average validation accuracy 0.9182\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0137\t Accuracy 0.9300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0148\t Accuracy 0.8912\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0150\t Accuracy 0.8847\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0152\t Accuracy 0.8787\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0153\t Accuracy 0.8798\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0152\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0151\t Accuracy 0.8813\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0153\t Accuracy 0.8794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0153\t Accuracy 0.8796\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0153\t Accuracy 0.8792\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0154\t Accuracy 0.8788\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0153\t Average training accuracy 0.8791\n",
      "Epoch [17]\t Average validation loss 0.0133\t Average validation accuracy 0.9188\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0137\t Accuracy 0.9300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0147\t Accuracy 0.8916\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0149\t Accuracy 0.8849\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0152\t Accuracy 0.8791\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0152\t Accuracy 0.8801\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0151\t Accuracy 0.8803\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0150\t Accuracy 0.8816\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0152\t Accuracy 0.8797\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0152\t Accuracy 0.8799\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0153\t Accuracy 0.8795\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0153\t Accuracy 0.8791\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0153\t Average training accuracy 0.8794\n",
      "Epoch [18]\t Average validation loss 0.0133\t Average validation accuracy 0.9186\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0136\t Accuracy 0.9300\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0147\t Accuracy 0.8924\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0148\t Accuracy 0.8851\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0151\t Accuracy 0.8794\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0152\t Accuracy 0.8803\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0151\t Accuracy 0.8806\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0150\t Accuracy 0.8818\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0152\t Accuracy 0.8799\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0152\t Accuracy 0.8802\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0152\t Accuracy 0.8798\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0152\t Accuracy 0.8794\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0152\t Average training accuracy 0.8796\n",
      "Epoch [19]\t Average validation loss 0.0132\t Average validation accuracy 0.9186\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9005.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2810\t Accuracy 0.1600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0776\t Accuracy 0.4531\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0530\t Accuracy 0.5871\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0432\t Accuracy 0.6501\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0375\t Accuracy 0.6934\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0336\t Accuracy 0.7254\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0308\t Accuracy 0.7480\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0287\t Accuracy 0.7652\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0270\t Accuracy 0.7794\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0256\t Accuracy 0.7914\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0245\t Accuracy 0.8006\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0235\t Average training accuracy 0.8087\n",
      "Epoch [0]\t Average validation loss 0.0114\t Average validation accuracy 0.9238\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0121\t Accuracy 0.9300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0124\t Accuracy 0.9092\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0125\t Accuracy 0.9056\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0126\t Accuracy 0.9015\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0125\t Accuracy 0.9031\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0123\t Accuracy 0.9048\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0122\t Accuracy 0.9055\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0122\t Accuracy 0.9055\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0121\t Accuracy 0.9059\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0120\t Accuracy 0.9068\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0120\t Accuracy 0.9070\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0118\t Average training accuracy 0.9076\n",
      "Epoch [1]\t Average validation loss 0.0093\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0098\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0102\t Accuracy 0.9249\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0103\t Accuracy 0.9213\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0104\t Accuracy 0.9191\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0104\t Accuracy 0.9204\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0103\t Accuracy 0.9210\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0103\t Accuracy 0.9214\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0103\t Accuracy 0.9208\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0103\t Accuracy 0.9206\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0102\t Accuracy 0.9213\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0102\t Accuracy 0.9207\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0102\t Average training accuracy 0.9211\n",
      "Epoch [2]\t Average validation loss 0.0083\t Average validation accuracy 0.9466\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0087\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0091\t Accuracy 0.9329\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0092\t Accuracy 0.9298\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0094\t Accuracy 0.9274\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0094\t Accuracy 0.9284\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0093\t Accuracy 0.9288\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0093\t Accuracy 0.9286\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0094\t Accuracy 0.9280\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0094\t Accuracy 0.9280\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0093\t Accuracy 0.9285\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0094\t Accuracy 0.9277\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0093\t Average training accuracy 0.9278\n",
      "Epoch [3]\t Average validation loss 0.0078\t Average validation accuracy 0.9520\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0080\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0085\t Accuracy 0.9384\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0086\t Accuracy 0.9353\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0088\t Accuracy 0.9338\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0088\t Accuracy 0.9342\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0088\t Accuracy 0.9342\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0088\t Accuracy 0.9340\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0088\t Accuracy 0.9334\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0088\t Accuracy 0.9333\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0088\t Accuracy 0.9337\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0089\t Accuracy 0.9327\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0088\t Average training accuracy 0.9327\n",
      "Epoch [4]\t Average validation loss 0.0074\t Average validation accuracy 0.9566\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0075\t Accuracy 0.9600\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0081\t Accuracy 0.9412\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0083\t Accuracy 0.9377\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0084\t Accuracy 0.9363\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0084\t Accuracy 0.9369\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0084\t Accuracy 0.9370\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0084\t Accuracy 0.9367\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0084\t Accuracy 0.9364\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0085\t Accuracy 0.9365\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0085\t Accuracy 0.9367\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0085\t Accuracy 0.9358\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0085\t Average training accuracy 0.9358\n",
      "Epoch [5]\t Average validation loss 0.0072\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0072\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0078\t Accuracy 0.9457\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0080\t Accuracy 0.9408\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0082\t Accuracy 0.9392\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0081\t Accuracy 0.9397\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0081\t Accuracy 0.9394\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0081\t Accuracy 0.9392\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0082\t Accuracy 0.9387\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0082\t Accuracy 0.9389\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0082\t Accuracy 0.9392\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0082\t Accuracy 0.9383\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0082\t Average training accuracy 0.9382\n",
      "Epoch [6]\t Average validation loss 0.0070\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0071\t Accuracy 0.9600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0076\t Accuracy 0.9475\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0078\t Accuracy 0.9423\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0080\t Accuracy 0.9406\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0079\t Accuracy 0.9412\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0079\t Accuracy 0.9410\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0079\t Accuracy 0.9411\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0080\t Accuracy 0.9406\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0080\t Accuracy 0.9408\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0080\t Accuracy 0.9411\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0081\t Accuracy 0.9401\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0080\t Average training accuracy 0.9400\n",
      "Epoch [7]\t Average validation loss 0.0069\t Average validation accuracy 0.9608\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0069\t Accuracy 0.9600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0074\t Accuracy 0.9486\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0076\t Accuracy 0.9434\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0078\t Accuracy 0.9417\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0078\t Accuracy 0.9427\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0078\t Accuracy 0.9425\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0078\t Accuracy 0.9425\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0078\t Accuracy 0.9421\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0078\t Accuracy 0.9422\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0079\t Accuracy 0.9425\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0079\t Accuracy 0.9416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0079\t Average training accuracy 0.9416\n",
      "Epoch [8]\t Average validation loss 0.0068\t Average validation accuracy 0.9616\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0069\t Accuracy 0.9600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0073\t Accuracy 0.9494\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0075\t Accuracy 0.9451\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0077\t Accuracy 0.9434\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0076\t Accuracy 0.9442\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0076\t Accuracy 0.9439\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0077\t Accuracy 0.9439\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0077\t Accuracy 0.9436\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0077\t Accuracy 0.9438\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0077\t Accuracy 0.9440\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0078\t Accuracy 0.9430\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0078\t Average training accuracy 0.9430\n",
      "Epoch [9]\t Average validation loss 0.0067\t Average validation accuracy 0.9616\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0068\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0072\t Accuracy 0.9512\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0074\t Accuracy 0.9467\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0076\t Accuracy 0.9453\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0075\t Accuracy 0.9458\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0075\t Accuracy 0.9455\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0076\t Accuracy 0.9455\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0076\t Accuracy 0.9450\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0076\t Accuracy 0.9452\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0076\t Accuracy 0.9454\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0077\t Accuracy 0.9444\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0077\t Average training accuracy 0.9443\n",
      "Epoch [10]\t Average validation loss 0.0066\t Average validation accuracy 0.9624\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0067\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0071\t Accuracy 0.9522\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0073\t Accuracy 0.9473\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0075\t Accuracy 0.9460\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0075\t Accuracy 0.9468\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0074\t Accuracy 0.9465\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0075\t Accuracy 0.9464\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0075\t Accuracy 0.9460\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0075\t Accuracy 0.9462\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0076\t Accuracy 0.9465\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0076\t Accuracy 0.9455\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0076\t Average training accuracy 0.9454\n",
      "Epoch [11]\t Average validation loss 0.0066\t Average validation accuracy 0.9630\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0067\t Accuracy 0.9600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0070\t Accuracy 0.9531\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0072\t Accuracy 0.9483\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0074\t Accuracy 0.9470\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0074\t Accuracy 0.9479\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0074\t Accuracy 0.9476\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0074\t Accuracy 0.9474\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0074\t Accuracy 0.9470\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0075\t Accuracy 0.9472\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0075\t Accuracy 0.9474\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0075\t Accuracy 0.9465\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0075\t Average training accuracy 0.9465\n",
      "Epoch [12]\t Average validation loss 0.0065\t Average validation accuracy 0.9630\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0070\t Accuracy 0.9539\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0072\t Accuracy 0.9490\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0073\t Accuracy 0.9481\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0073\t Accuracy 0.9488\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0073\t Accuracy 0.9484\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0074\t Accuracy 0.9481\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0074\t Accuracy 0.9477\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0074\t Accuracy 0.9479\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0074\t Accuracy 0.9482\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0075\t Accuracy 0.9473\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0074\t Average training accuracy 0.9473\n",
      "Epoch [13]\t Average validation loss 0.0065\t Average validation accuracy 0.9642\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0069\t Accuracy 0.9553\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0071\t Accuracy 0.9498\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0073\t Accuracy 0.9489\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0073\t Accuracy 0.9497\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0073\t Accuracy 0.9491\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0073\t Accuracy 0.9488\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0073\t Accuracy 0.9484\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9487\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0074\t Accuracy 0.9490\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0074\t Accuracy 0.9482\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0074\t Average training accuracy 0.9482\n",
      "Epoch [14]\t Average validation loss 0.0064\t Average validation accuracy 0.9648\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0069\t Accuracy 0.9555\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0071\t Accuracy 0.9503\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9493\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0072\t Accuracy 0.9503\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0072\t Accuracy 0.9499\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0073\t Accuracy 0.9496\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0073\t Accuracy 0.9491\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9494\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0073\t Accuracy 0.9496\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0074\t Accuracy 0.9487\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0073\t Average training accuracy 0.9487\n",
      "Epoch [15]\t Average validation loss 0.0064\t Average validation accuracy 0.9648\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9567\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9512\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9501\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0072\t Accuracy 0.9511\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0072\t Accuracy 0.9507\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9502\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9497\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0073\t Accuracy 0.9502\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0073\t Accuracy 0.9493\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0073\t Average training accuracy 0.9492\n",
      "Epoch [16]\t Average validation loss 0.0064\t Average validation accuracy 0.9652\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9567\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9514\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9501\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9513\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9510\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9502\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9504\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0073\t Accuracy 0.9496\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0073\t Average training accuracy 0.9496\n",
      "Epoch [17]\t Average validation loss 0.0063\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9569\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9516\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9504\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9517\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9512\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9508\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9503\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9505\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9506\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9499\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0072\t Average training accuracy 0.9499\n",
      "Epoch [18]\t Average validation loss 0.0063\t Average validation accuracy 0.9664\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0067\t Accuracy 0.9573\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0069\t Accuracy 0.9520\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9506\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9520\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9517\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9511\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0071\t Accuracy 0.9508\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9510\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9511\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9503\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0072\t Average training accuracy 0.9503\n",
      "Epoch [19]\t Average validation loss 0.0063\t Average validation accuracy 0.9670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9533.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2915\t Accuracy 0.1400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0759\t Accuracy 0.4469\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0517\t Accuracy 0.5890\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0420\t Accuracy 0.6610\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0364\t Accuracy 0.7037\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0325\t Accuracy 0.7336\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0298\t Accuracy 0.7549\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0277\t Accuracy 0.7708\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0260\t Accuracy 0.7845\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0247\t Accuracy 0.7956\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0235\t Accuracy 0.8046\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0225\t Average training accuracy 0.8123\n",
      "Epoch [0]\t Average validation loss 0.0105\t Average validation accuracy 0.9254\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0105\t Accuracy 0.9400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0113\t Accuracy 0.9131\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0114\t Accuracy 0.9085\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0116\t Accuracy 0.9046\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0114\t Accuracy 0.9066\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0114\t Accuracy 0.9065\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0113\t Accuracy 0.9070\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0113\t Accuracy 0.9065\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0112\t Accuracy 0.9070\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0111\t Accuracy 0.9078\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0111\t Accuracy 0.9079\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0110\t Average training accuracy 0.9085\n",
      "Epoch [1]\t Average validation loss 0.0086\t Average validation accuracy 0.9404\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0086\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0094\t Accuracy 0.9292\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0096\t Accuracy 0.9235\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0098\t Accuracy 0.9203\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0097\t Accuracy 0.9214\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0097\t Accuracy 0.9207\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0097\t Accuracy 0.9204\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0097\t Accuracy 0.9201\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0097\t Accuracy 0.9204\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0097\t Accuracy 0.9209\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0097\t Accuracy 0.9205\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0097\t Average training accuracy 0.9209\n",
      "Epoch [2]\t Average validation loss 0.0079\t Average validation accuracy 0.9466\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0078\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0086\t Accuracy 0.9347\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0088\t Accuracy 0.9306\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0090\t Accuracy 0.9281\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0090\t Accuracy 0.9293\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0089\t Accuracy 0.9290\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0090\t Accuracy 0.9284\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0090\t Accuracy 0.9279\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0090\t Accuracy 0.9278\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0090\t Accuracy 0.9279\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0090\t Accuracy 0.9274\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0090\t Average training accuracy 0.9275\n",
      "Epoch [3]\t Average validation loss 0.0075\t Average validation accuracy 0.9526\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0074\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0081\t Accuracy 0.9384\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0083\t Accuracy 0.9340\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0085\t Accuracy 0.9325\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0085\t Accuracy 0.9338\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0085\t Accuracy 0.9334\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0085\t Accuracy 0.9329\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0085\t Accuracy 0.9324\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0085\t Accuracy 0.9324\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0085\t Accuracy 0.9326\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0086\t Accuracy 0.9319\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0086\t Average training accuracy 0.9319\n",
      "Epoch [4]\t Average validation loss 0.0072\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0071\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0078\t Accuracy 0.9416\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0080\t Accuracy 0.9372\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0082\t Accuracy 0.9352\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0082\t Accuracy 0.9370\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0082\t Accuracy 0.9367\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0082\t Accuracy 0.9362\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0082\t Accuracy 0.9358\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0082\t Accuracy 0.9358\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0082\t Accuracy 0.9358\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0083\t Accuracy 0.9350\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0083\t Average training accuracy 0.9349\n",
      "Epoch [5]\t Average validation loss 0.0070\t Average validation accuracy 0.9570\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0069\t Accuracy 0.9600\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0076\t Accuracy 0.9433\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0078\t Accuracy 0.9393\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0080\t Accuracy 0.9373\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0079\t Accuracy 0.9390\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0079\t Accuracy 0.9390\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0080\t Accuracy 0.9383\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0080\t Accuracy 0.9379\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0080\t Accuracy 0.9380\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0080\t Accuracy 0.9380\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0081\t Accuracy 0.9372\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0081\t Average training accuracy 0.9372\n",
      "Epoch [6]\t Average validation loss 0.0068\t Average validation accuracy 0.9586\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0068\t Accuracy 0.9600\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0074\t Accuracy 0.9459\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0076\t Accuracy 0.9415\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0078\t Accuracy 0.9401\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0078\t Accuracy 0.9416\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0078\t Accuracy 0.9414\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0078\t Accuracy 0.9408\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0078\t Accuracy 0.9403\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0078\t Accuracy 0.9403\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0079\t Accuracy 0.9402\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0079\t Accuracy 0.9394\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0079\t Average training accuracy 0.9394\n",
      "Epoch [7]\t Average validation loss 0.0067\t Average validation accuracy 0.9612\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0067\t Accuracy 0.9600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0073\t Accuracy 0.9473\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0075\t Accuracy 0.9429\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0077\t Accuracy 0.9415\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0076\t Accuracy 0.9432\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0076\t Accuracy 0.9429\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0077\t Accuracy 0.9424\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0077\t Accuracy 0.9419\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0077\t Accuracy 0.9419\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0077\t Accuracy 0.9419\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0078\t Accuracy 0.9412\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0077\t Average training accuracy 0.9411\n",
      "Epoch [8]\t Average validation loss 0.0066\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0067\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0072\t Accuracy 0.9492\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0074\t Accuracy 0.9451\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0076\t Accuracy 0.9436\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0075\t Accuracy 0.9451\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0075\t Accuracy 0.9447\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0075\t Accuracy 0.9442\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0076\t Accuracy 0.9436\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0076\t Accuracy 0.9437\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0076\t Accuracy 0.9438\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0077\t Accuracy 0.9430\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0076\t Average training accuracy 0.9429\n",
      "Epoch [9]\t Average validation loss 0.0065\t Average validation accuracy 0.9622\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0071\t Accuracy 0.9516\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0073\t Accuracy 0.9468\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0075\t Accuracy 0.9455\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0074\t Accuracy 0.9466\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0074\t Accuracy 0.9461\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0075\t Accuracy 0.9455\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0075\t Accuracy 0.9450\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0075\t Accuracy 0.9451\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0075\t Accuracy 0.9451\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0076\t Accuracy 0.9442\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0075\t Average training accuracy 0.9442\n",
      "Epoch [10]\t Average validation loss 0.0065\t Average validation accuracy 0.9626\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0070\t Accuracy 0.9520\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0072\t Accuracy 0.9474\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0074\t Accuracy 0.9464\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0073\t Accuracy 0.9475\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0073\t Accuracy 0.9470\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0074\t Accuracy 0.9465\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0074\t Accuracy 0.9462\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0074\t Accuracy 0.9461\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0074\t Accuracy 0.9461\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0075\t Accuracy 0.9452\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0075\t Average training accuracy 0.9452\n",
      "Epoch [11]\t Average validation loss 0.0064\t Average validation accuracy 0.9630\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0069\t Accuracy 0.9529\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0071\t Accuracy 0.9483\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0073\t Accuracy 0.9475\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0073\t Accuracy 0.9486\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0072\t Accuracy 0.9480\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0073\t Accuracy 0.9476\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0073\t Accuracy 0.9473\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9473\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0074\t Accuracy 0.9473\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0074\t Accuracy 0.9464\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0074\t Average training accuracy 0.9464\n",
      "Epoch [12]\t Average validation loss 0.0064\t Average validation accuracy 0.9642\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0066\t Accuracy 0.9600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9539\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0071\t Accuracy 0.9492\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9481\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0072\t Accuracy 0.9493\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0072\t Accuracy 0.9487\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9483\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0073\t Accuracy 0.9481\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9480\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0073\t Accuracy 0.9481\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0073\t Accuracy 0.9473\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0073\t Average training accuracy 0.9473\n",
      "Epoch [13]\t Average validation loss 0.0064\t Average validation accuracy 0.9650\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9551\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9504\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9491\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9502\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9496\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9492\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9489\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9490\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9491\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0073\t Accuracy 0.9482\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0073\t Average training accuracy 0.9482\n",
      "Epoch [14]\t Average validation loss 0.0063\t Average validation accuracy 0.9654\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9553\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9510\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9497\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9509\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9504\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9499\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9496\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9497\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9497\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9488\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0072\t Average training accuracy 0.9488\n",
      "Epoch [15]\t Average validation loss 0.0063\t Average validation accuracy 0.9656\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0067\t Accuracy 0.9559\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0069\t Accuracy 0.9516\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9505\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9517\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9511\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9505\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0071\t Accuracy 0.9503\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0071\t Accuracy 0.9503\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9504\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9496\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0072\t Average training accuracy 0.9495\n",
      "Epoch [16]\t Average validation loss 0.0063\t Average validation accuracy 0.9656\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0067\t Accuracy 0.9563\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0069\t Accuracy 0.9520\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9509\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0070\t Accuracy 0.9522\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0070\t Accuracy 0.9516\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9510\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0071\t Accuracy 0.9507\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0071\t Accuracy 0.9508\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0071\t Accuracy 0.9509\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17]\t Average training loss 0.0072\t Average training accuracy 0.9501\n",
      "Epoch [17]\t Average validation loss 0.0062\t Average validation accuracy 0.9656\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0065\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0067\t Accuracy 0.9569\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0069\t Accuracy 0.9528\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0070\t Accuracy 0.9517\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0070\t Accuracy 0.9529\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0070\t Accuracy 0.9524\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0070\t Accuracy 0.9516\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0071\t Accuracy 0.9513\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0071\t Accuracy 0.9513\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0071\t Accuracy 0.9514\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0071\t Accuracy 0.9507\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0071\t Average training accuracy 0.9505\n",
      "Epoch [18]\t Average validation loss 0.0062\t Average validation accuracy 0.9658\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0064\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0066\t Accuracy 0.9575\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0068\t Accuracy 0.9532\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0070\t Accuracy 0.9521\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0070\t Accuracy 0.9533\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0070\t Accuracy 0.9527\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0070\t Accuracy 0.9519\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0070\t Accuracy 0.9517\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0071\t Accuracy 0.9516\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0071\t Accuracy 0.9517\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0071\t Accuracy 0.9509\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0071\t Average training accuracy 0.9508\n",
      "Epoch [19]\t Average validation loss 0.0062\t Average validation accuracy 0.9658\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9514.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigmoid_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-124d7e42306d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n\u001b[0m\u001b[1;32m      2\u001b[0m                    'relu': [relu_loss, relu_acc]})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sigmoid_loss' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework-2, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of experiment report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 256))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(256, 64))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.6131\t Accuracy 0.0400\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0932\t Accuracy 0.3598\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0628\t Accuracy 0.5062\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0508\t Accuracy 0.5837\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0440\t Accuracy 0.6347\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0393\t Accuracy 0.6744\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0358\t Accuracy 0.7042\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0332\t Accuracy 0.7258\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0310\t Accuracy 0.7441\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0292\t Accuracy 0.7594\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0277\t Accuracy 0.7724\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0264\t Average training accuracy 0.7841\n",
      "Epoch [0]\t Average validation loss 0.0110\t Average validation accuracy 0.9274\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0118\t Accuracy 0.9300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0116\t Accuracy 0.9173\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0117\t Accuracy 0.9142\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0117\t Accuracy 0.9130\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0115\t Accuracy 0.9140\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0113\t Accuracy 0.9153\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0112\t Accuracy 0.9157\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0111\t Accuracy 0.9160\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0110\t Accuracy 0.9169\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0108\t Accuracy 0.9175\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0108\t Accuracy 0.9175\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0106\t Average training accuracy 0.9182\n",
      "Epoch [1]\t Average validation loss 0.0080\t Average validation accuracy 0.9502\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0088\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0087\t Accuracy 0.9371\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0088\t Accuracy 0.9335\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0090\t Accuracy 0.9326\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0089\t Accuracy 0.9330\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0089\t Accuracy 0.9333\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0088\t Accuracy 0.9329\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0088\t Accuracy 0.9328\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0088\t Accuracy 0.9334\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0087\t Accuracy 0.9334\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0088\t Accuracy 0.9328\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0087\t Average training accuracy 0.9331\n",
      "Epoch [2]\t Average validation loss 0.0070\t Average validation accuracy 0.9568\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0075\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0076\t Accuracy 0.9459\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0077\t Accuracy 0.9421\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0079\t Accuracy 0.9414\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0078\t Accuracy 0.9417\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0078\t Accuracy 0.9414\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0078\t Accuracy 0.9412\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0078\t Accuracy 0.9409\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0078\t Accuracy 0.9414\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0078\t Accuracy 0.9416\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0078\t Accuracy 0.9409\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0078\t Average training accuracy 0.9409\n",
      "Epoch [3]\t Average validation loss 0.0064\t Average validation accuracy 0.9622\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0069\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0069\t Accuracy 0.9508\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0071\t Accuracy 0.9471\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9465\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0072\t Accuracy 0.9468\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0072\t Accuracy 0.9464\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9458\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0073\t Accuracy 0.9456\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9458\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0073\t Accuracy 0.9461\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0073\t Accuracy 0.9455\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0073\t Average training accuracy 0.9455\n",
      "Epoch [4]\t Average validation loss 0.0060\t Average validation accuracy 0.9650\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0064\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0065\t Accuracy 0.9555\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0067\t Accuracy 0.9513\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0069\t Accuracy 0.9505\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0068\t Accuracy 0.9503\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0068\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0069\t Accuracy 0.9494\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0069\t Accuracy 0.9492\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0069\t Accuracy 0.9494\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0069\t Accuracy 0.9496\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0069\t Accuracy 0.9492\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0069\t Average training accuracy 0.9493\n",
      "Epoch [5]\t Average validation loss 0.0058\t Average validation accuracy 0.9668\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0062\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0063\t Accuracy 0.9582\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0065\t Accuracy 0.9539\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0066\t Accuracy 0.9525\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0066\t Accuracy 0.9523\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0066\t Accuracy 0.9520\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0066\t Accuracy 0.9513\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0066\t Accuracy 0.9511\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0066\t Accuracy 0.9512\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0066\t Accuracy 0.9516\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0067\t Accuracy 0.9510\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0067\t Average training accuracy 0.9512\n",
      "Epoch [6]\t Average validation loss 0.0056\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0060\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0061\t Accuracy 0.9614\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0063\t Accuracy 0.9565\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0064\t Accuracy 0.9552\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0064\t Accuracy 0.9549\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0064\t Accuracy 0.9546\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0064\t Accuracy 0.9538\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0064\t Accuracy 0.9535\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0065\t Accuracy 0.9536\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0065\t Accuracy 0.9539\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0065\t Accuracy 0.9532\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0065\t Average training accuracy 0.9533\n",
      "Epoch [7]\t Average validation loss 0.0055\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0058\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0059\t Accuracy 0.9624\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0061\t Accuracy 0.9576\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0062\t Accuracy 0.9566\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0062\t Accuracy 0.9562\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0062\t Accuracy 0.9557\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0063\t Accuracy 0.9549\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0063\t Accuracy 0.9547\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0063\t Accuracy 0.9548\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0063\t Accuracy 0.9550\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0063\t Accuracy 0.9542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0063\t Average training accuracy 0.9543\n",
      "Epoch [8]\t Average validation loss 0.0053\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0056\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0058\t Accuracy 0.9627\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0060\t Accuracy 0.9584\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0061\t Accuracy 0.9574\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0061\t Accuracy 0.9571\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0061\t Accuracy 0.9568\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0061\t Accuracy 0.9561\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0062\t Accuracy 0.9560\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0062\t Accuracy 0.9561\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0062\t Accuracy 0.9561\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0062\t Accuracy 0.9553\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0062\t Average training accuracy 0.9554\n",
      "Epoch [9]\t Average validation loss 0.0052\t Average validation accuracy 0.9694\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0055\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0057\t Accuracy 0.9629\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0059\t Accuracy 0.9594\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0060\t Accuracy 0.9583\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0060\t Accuracy 0.9581\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0060\t Accuracy 0.9577\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0060\t Accuracy 0.9569\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0060\t Accuracy 0.9568\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0060\t Accuracy 0.9569\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0061\t Accuracy 0.9569\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0061\t Accuracy 0.9560\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0061\t Average training accuracy 0.9561\n",
      "Epoch [10]\t Average validation loss 0.0051\t Average validation accuracy 0.9694\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0054\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0056\t Accuracy 0.9655\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0058\t Accuracy 0.9610\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0059\t Accuracy 0.9596\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0059\t Accuracy 0.9591\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0059\t Accuracy 0.9588\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0059\t Accuracy 0.9578\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0059\t Accuracy 0.9579\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0060\t Accuracy 0.9578\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0060\t Accuracy 0.9577\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0060\t Accuracy 0.9569\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0060\t Average training accuracy 0.9569\n",
      "Epoch [11]\t Average validation loss 0.0051\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0053\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0055\t Accuracy 0.9653\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0057\t Accuracy 0.9611\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0058\t Accuracy 0.9596\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0058\t Accuracy 0.9592\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0058\t Accuracy 0.9591\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0058\t Accuracy 0.9582\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0059\t Accuracy 0.9584\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0059\t Accuracy 0.9583\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0059\t Accuracy 0.9583\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0059\t Accuracy 0.9574\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0059\t Average training accuracy 0.9575\n",
      "Epoch [12]\t Average validation loss 0.0050\t Average validation accuracy 0.9704\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0052\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0054\t Accuracy 0.9663\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0056\t Accuracy 0.9619\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0057\t Accuracy 0.9603\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0057\t Accuracy 0.9598\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0057\t Accuracy 0.9597\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0058\t Accuracy 0.9587\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0058\t Accuracy 0.9589\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0058\t Accuracy 0.9589\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0058\t Accuracy 0.9588\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0058\t Accuracy 0.9580\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0058\t Average training accuracy 0.9580\n",
      "Epoch [13]\t Average validation loss 0.0049\t Average validation accuracy 0.9706\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0051\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0053\t Accuracy 0.9657\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0055\t Accuracy 0.9618\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0056\t Accuracy 0.9605\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0056\t Accuracy 0.9602\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0056\t Accuracy 0.9601\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0057\t Accuracy 0.9591\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0057\t Accuracy 0.9593\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0057\t Accuracy 0.9593\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0057\t Accuracy 0.9592\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0057\t Accuracy 0.9584\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0057\t Average training accuracy 0.9584\n",
      "Epoch [14]\t Average validation loss 0.0048\t Average validation accuracy 0.9706\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0051\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0052\t Accuracy 0.9665\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0055\t Accuracy 0.9622\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0056\t Accuracy 0.9610\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0056\t Accuracy 0.9607\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0055\t Accuracy 0.9608\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0056\t Accuracy 0.9599\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0056\t Accuracy 0.9600\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0056\t Accuracy 0.9599\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0056\t Accuracy 0.9598\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0057\t Accuracy 0.9590\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0057\t Average training accuracy 0.9590\n",
      "Epoch [15]\t Average validation loss 0.0048\t Average validation accuracy 0.9708\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0050\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0052\t Accuracy 0.9669\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0054\t Accuracy 0.9626\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0055\t Accuracy 0.9615\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0055\t Accuracy 0.9612\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0055\t Accuracy 0.9612\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0055\t Accuracy 0.9603\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0056\t Accuracy 0.9606\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0056\t Accuracy 0.9606\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0056\t Accuracy 0.9605\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0056\t Accuracy 0.9597\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0056\t Average training accuracy 0.9597\n",
      "Epoch [16]\t Average validation loss 0.0047\t Average validation accuracy 0.9710\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0050\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0051\t Accuracy 0.9673\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0054\t Accuracy 0.9629\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0055\t Accuracy 0.9619\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0055\t Accuracy 0.9616\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0054\t Accuracy 0.9616\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0055\t Accuracy 0.9607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0055\t Accuracy 0.9609\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0055\t Accuracy 0.9609\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0055\t Accuracy 0.9608\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0056\t Accuracy 0.9600\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0056\t Average training accuracy 0.9600\n",
      "Epoch [17]\t Average validation loss 0.0047\t Average validation accuracy 0.9712\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0050\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0051\t Accuracy 0.9675\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0053\t Accuracy 0.9635\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0054\t Accuracy 0.9625\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0054\t Accuracy 0.9623\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0054\t Accuracy 0.9624\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0055\t Accuracy 0.9613\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0055\t Accuracy 0.9615\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0055\t Accuracy 0.9615\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0055\t Accuracy 0.9614\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0055\t Accuracy 0.9606\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0055\t Average training accuracy 0.9606\n",
      "Epoch [18]\t Average validation loss 0.0047\t Average validation accuracy 0.9714\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0049\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0051\t Accuracy 0.9676\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0053\t Accuracy 0.9637\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0054\t Accuracy 0.9627\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0054\t Accuracy 0.9625\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0054\t Accuracy 0.9625\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0054\t Accuracy 0.9616\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0055\t Accuracy 0.9617\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0055\t Accuracy 0.9618\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0055\t Accuracy 0.9616\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0055\t Accuracy 0.9608\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0055\t Average training accuracy 0.9607\n",
      "Epoch [19]\t Average validation loss 0.0047\t Average validation accuracy 0.9712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9613.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 4 6 1]\n",
      " [1 5 2 9]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[2,4,6,1],[1,5,2,9]])\n",
    "print(a)\n",
    "np.argmax(a,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
