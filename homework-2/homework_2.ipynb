{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### **Deadline: 2018.11.04 23:59:59**\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.5\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.3377\t Accuracy 0.0700\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.4856\t Accuracy 0.1712\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2674\t Accuracy 0.2347\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.1926\t Accuracy 0.2830\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1539\t Accuracy 0.3436\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1299\t Accuracy 0.3937\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.1134\t Accuracy 0.4366\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.1014\t Accuracy 0.4766\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0922\t Accuracy 0.5112\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0849\t Accuracy 0.5398\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0789\t Accuracy 0.5640\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0740\t Average training accuracy 0.5848\n",
      "Epoch [0]\t Average validation loss 0.0225\t Average validation accuracy 0.8328\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0215\t Accuracy 0.8400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0232\t Accuracy 0.8078\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0230\t Accuracy 0.8183\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0230\t Accuracy 0.8143\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0228\t Accuracy 0.8186\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0226\t Accuracy 0.8219\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0224\t Accuracy 0.8244\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0223\t Accuracy 0.8252\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0221\t Accuracy 0.8281\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0220\t Accuracy 0.8295\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0219\t Accuracy 0.8305\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0217\t Average training accuracy 0.8324\n",
      "Epoch [1]\t Average validation loss 0.0184\t Average validation accuracy 0.8872\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0180\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0196\t Accuracy 0.8561\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0196\t Accuracy 0.8560\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0198\t Accuracy 0.8496\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0198\t Accuracy 0.8515\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0197\t Accuracy 0.8530\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0197\t Accuracy 0.8539\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0197\t Accuracy 0.8533\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0197\t Accuracy 0.8548\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0196\t Accuracy 0.8548\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0196\t Accuracy 0.8548\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0195\t Average training accuracy 0.8554\n",
      "Epoch [2]\t Average validation loss 0.0172\t Average validation accuracy 0.8994\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0172\t Accuracy 0.9100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0184\t Accuracy 0.8678\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0184\t Accuracy 0.8649\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0187\t Accuracy 0.8589\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0187\t Accuracy 0.8609\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0187\t Accuracy 0.8616\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0187\t Accuracy 0.8622\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0187\t Accuracy 0.8615\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0187\t Accuracy 0.8625\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0187\t Accuracy 0.8627\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0187\t Accuracy 0.8625\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0186\t Average training accuracy 0.8627\n",
      "Epoch [3]\t Average validation loss 0.0166\t Average validation accuracy 0.9030\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0167\t Accuracy 0.9100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0177\t Accuracy 0.8727\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0177\t Accuracy 0.8693\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0180\t Accuracy 0.8635\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0180\t Accuracy 0.8652\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0180\t Accuracy 0.8659\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0180\t Accuracy 0.8665\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0181\t Accuracy 0.8659\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0180\t Accuracy 0.8665\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0180\t Accuracy 0.8669\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0180\t Accuracy 0.8667\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0180\t Average training accuracy 0.8670\n",
      "Epoch [4]\t Average validation loss 0.0162\t Average validation accuracy 0.9044\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0164\t Accuracy 0.9200\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0171\t Accuracy 0.8759\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0171\t Accuracy 0.8725\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0174\t Accuracy 0.8670\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0174\t Accuracy 0.8684\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0174\t Accuracy 0.8691\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0174\t Accuracy 0.8695\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0175\t Accuracy 0.8683\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0174\t Accuracy 0.8693\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0175\t Accuracy 0.8693\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0175\t Accuracy 0.8692\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0174\t Average training accuracy 0.8695\n",
      "Epoch [5]\t Average validation loss 0.0158\t Average validation accuracy 0.9086\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0160\t Accuracy 0.9200\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0166\t Accuracy 0.8792\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0167\t Accuracy 0.8750\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0169\t Accuracy 0.8695\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0169\t Accuracy 0.8712\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0169\t Accuracy 0.8716\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0169\t Accuracy 0.8716\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0170\t Accuracy 0.8701\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0170\t Accuracy 0.8709\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0170\t Accuracy 0.8706\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0170\t Accuracy 0.8705\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0170\t Average training accuracy 0.8706\n",
      "Epoch [6]\t Average validation loss 0.0154\t Average validation accuracy 0.9116\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0157\t Accuracy 0.9300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0163\t Accuracy 0.8806\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0164\t Accuracy 0.8752\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0167\t Accuracy 0.8700\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0167\t Accuracy 0.8718\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0167\t Accuracy 0.8714\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0166\t Accuracy 0.8718\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0168\t Accuracy 0.8699\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0168\t Accuracy 0.8704\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0168\t Accuracy 0.8702\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0168\t Accuracy 0.8701\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0168\t Average training accuracy 0.8703\n",
      "Epoch [7]\t Average validation loss 0.0150\t Average validation accuracy 0.9132\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0153\t Accuracy 0.9400\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0162\t Accuracy 0.8820\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0162\t Accuracy 0.8769\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0165\t Accuracy 0.8710\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0165\t Accuracy 0.8724\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0165\t Accuracy 0.8718\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0164\t Accuracy 0.8722\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0166\t Accuracy 0.8701\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0166\t Accuracy 0.8706\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0166\t Accuracy 0.8705\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0166\t Accuracy 0.8704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0166\t Average training accuracy 0.8707\n",
      "Epoch [8]\t Average validation loss 0.0147\t Average validation accuracy 0.9150\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0148\t Accuracy 0.9400\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0159\t Accuracy 0.8843\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0160\t Accuracy 0.8780\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0163\t Accuracy 0.8723\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0163\t Accuracy 0.8737\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0163\t Accuracy 0.8730\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0162\t Accuracy 0.8735\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0164\t Accuracy 0.8717\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0163\t Accuracy 0.8720\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0164\t Accuracy 0.8718\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0164\t Accuracy 0.8716\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0164\t Average training accuracy 0.8718\n",
      "Epoch [9]\t Average validation loss 0.0143\t Average validation accuracy 0.9164\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0145\t Accuracy 0.9400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0157\t Accuracy 0.8849\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0158\t Accuracy 0.8786\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0161\t Accuracy 0.8726\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0161\t Accuracy 0.8742\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0160\t Accuracy 0.8740\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0159\t Accuracy 0.8746\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0161\t Accuracy 0.8728\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0161\t Accuracy 0.8731\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0162\t Accuracy 0.8728\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0162\t Accuracy 0.8726\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0161\t Average training accuracy 0.8728\n",
      "Epoch [10]\t Average validation loss 0.0141\t Average validation accuracy 0.9172\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0142\t Accuracy 0.9300\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0155\t Accuracy 0.8865\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0156\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0159\t Accuracy 0.8740\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0159\t Accuracy 0.8756\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0158\t Accuracy 0.8756\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0157\t Accuracy 0.8761\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0159\t Accuracy 0.8743\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0159\t Accuracy 0.8747\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0160\t Accuracy 0.8744\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0160\t Accuracy 0.8741\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0160\t Average training accuracy 0.8742\n",
      "Epoch [11]\t Average validation loss 0.0138\t Average validation accuracy 0.9176\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0140\t Accuracy 0.9300\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0153\t Accuracy 0.8865\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0154\t Accuracy 0.8807\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0157\t Accuracy 0.8752\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0157\t Accuracy 0.8765\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0157\t Accuracy 0.8763\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0156\t Accuracy 0.8769\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0158\t Accuracy 0.8752\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0158\t Accuracy 0.8755\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0158\t Accuracy 0.8752\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0158\t Accuracy 0.8750\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0158\t Average training accuracy 0.8751\n",
      "Epoch [12]\t Average validation loss 0.0136\t Average validation accuracy 0.9174\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0139\t Accuracy 0.9300\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0151\t Accuracy 0.8880\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0153\t Accuracy 0.8817\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0156\t Accuracy 0.8761\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0156\t Accuracy 0.8774\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0155\t Accuracy 0.8774\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0154\t Accuracy 0.8781\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0156\t Accuracy 0.8764\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0156\t Accuracy 0.8768\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0157\t Accuracy 0.8765\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0157\t Accuracy 0.8763\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0157\t Average training accuracy 0.8765\n",
      "Epoch [13]\t Average validation loss 0.0135\t Average validation accuracy 0.9172\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0137\t Accuracy 0.9300\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0150\t Accuracy 0.8892\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0152\t Accuracy 0.8823\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0155\t Accuracy 0.8768\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0155\t Accuracy 0.8782\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0154\t Accuracy 0.8782\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0153\t Accuracy 0.8790\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0155\t Accuracy 0.8772\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0155\t Accuracy 0.8776\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0156\t Accuracy 0.8773\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0156\t Accuracy 0.8770\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0156\t Average training accuracy 0.8772\n",
      "Epoch [14]\t Average validation loss 0.0134\t Average validation accuracy 0.9178\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0136\t Accuracy 0.9300\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0149\t Accuracy 0.8914\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0151\t Accuracy 0.8836\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0154\t Accuracy 0.8781\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0154\t Accuracy 0.8792\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0153\t Accuracy 0.8792\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0152\t Accuracy 0.8799\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0154\t Accuracy 0.8779\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0154\t Accuracy 0.8783\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0155\t Accuracy 0.8779\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0155\t Accuracy 0.8776\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0155\t Average training accuracy 0.8779\n",
      "Epoch [15]\t Average validation loss 0.0133\t Average validation accuracy 0.9182\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0135\t Accuracy 0.9300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0148\t Accuracy 0.8914\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0150\t Accuracy 0.8840\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0153\t Accuracy 0.8785\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0153\t Accuracy 0.8797\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0153\t Accuracy 0.8798\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0152\t Accuracy 0.8804\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0154\t Accuracy 0.8785\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0154\t Accuracy 0.8787\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0154\t Accuracy 0.8784\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0154\t Accuracy 0.8781\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0154\t Average training accuracy 0.8784\n",
      "Epoch [16]\t Average validation loss 0.0132\t Average validation accuracy 0.9184\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0135\t Accuracy 0.9300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0147\t Accuracy 0.8920\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0149\t Accuracy 0.8844\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0152\t Accuracy 0.8789\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0153\t Accuracy 0.8800\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0152\t Accuracy 0.8802\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0151\t Accuracy 0.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0153\t Accuracy 0.8789\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0153\t Accuracy 0.8791\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0153\t Accuracy 0.8788\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0153\t Accuracy 0.8785\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0153\t Average training accuracy 0.8788\n",
      "Epoch [17]\t Average validation loss 0.0131\t Average validation accuracy 0.9188\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0134\t Accuracy 0.9300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0147\t Accuracy 0.8920\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0149\t Accuracy 0.8847\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0152\t Accuracy 0.8794\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0152\t Accuracy 0.8803\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0151\t Accuracy 0.8806\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0150\t Accuracy 0.8814\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0153\t Accuracy 0.8793\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0152\t Accuracy 0.8795\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0153\t Accuracy 0.8791\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0153\t Accuracy 0.8789\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0153\t Average training accuracy 0.8792\n",
      "Epoch [18]\t Average validation loss 0.0130\t Average validation accuracy 0.9190\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0134\t Accuracy 0.9300\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0146\t Accuracy 0.8918\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0148\t Accuracy 0.8847\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0151\t Accuracy 0.8795\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0152\t Accuracy 0.8805\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0151\t Accuracy 0.8808\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0150\t Accuracy 0.8816\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0152\t Accuracy 0.8795\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0152\t Accuracy 0.8796\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0152\t Accuracy 0.8793\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0152\t Accuracy 0.8790\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0152\t Average training accuracy 0.8792\n",
      "Epoch [19]\t Average validation loss 0.0130\t Average validation accuracy 0.9190\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9003.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2564\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.0671\t Accuracy 0.4947\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.0480\t Accuracy 0.6088\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.0402\t Accuracy 0.6685\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.0355\t Accuracy 0.7099\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.0322\t Accuracy 0.7389\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.0298\t Accuracy 0.7592\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.0279\t Accuracy 0.7742\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.0264\t Accuracy 0.7873\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.0251\t Accuracy 0.7989\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.0241\t Accuracy 0.8076\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0231\t Average training accuracy 0.8154\n",
      "Epoch [0]\t Average validation loss 0.0115\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0114\t Accuracy 0.9400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.0123\t Accuracy 0.9139\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.0123\t Accuracy 0.9107\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.0124\t Accuracy 0.9069\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.0123\t Accuracy 0.9099\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.0121\t Accuracy 0.9108\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.0120\t Accuracy 0.9115\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.0119\t Accuracy 0.9116\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.0119\t Accuracy 0.9120\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.0117\t Accuracy 0.9128\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.0117\t Accuracy 0.9124\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0116\t Average training accuracy 0.9131\n",
      "Epoch [1]\t Average validation loss 0.0091\t Average validation accuracy 0.9444\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0090\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0097\t Accuracy 0.9322\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0099\t Accuracy 0.9280\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0101\t Accuracy 0.9265\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0100\t Accuracy 0.9281\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0099\t Accuracy 0.9287\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0099\t Accuracy 0.9287\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0099\t Accuracy 0.9286\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0099\t Accuracy 0.9285\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0099\t Accuracy 0.9288\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0099\t Accuracy 0.9280\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0098\t Average training accuracy 0.9283\n",
      "Epoch [2]\t Average validation loss 0.0081\t Average validation accuracy 0.9510\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0082\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0087\t Accuracy 0.9400\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0089\t Accuracy 0.9357\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0091\t Accuracy 0.9334\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0091\t Accuracy 0.9349\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0090\t Accuracy 0.9355\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0090\t Accuracy 0.9355\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0091\t Accuracy 0.9350\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0091\t Accuracy 0.9351\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0091\t Accuracy 0.9353\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0091\t Accuracy 0.9345\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0090\t Average training accuracy 0.9347\n",
      "Epoch [3]\t Average validation loss 0.0076\t Average validation accuracy 0.9550\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0078\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0082\t Accuracy 0.9453\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0084\t Accuracy 0.9412\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0086\t Accuracy 0.9383\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0085\t Accuracy 0.9395\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0085\t Accuracy 0.9395\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0085\t Accuracy 0.9392\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0086\t Accuracy 0.9386\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0086\t Accuracy 0.9387\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0086\t Accuracy 0.9389\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0086\t Accuracy 0.9380\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0086\t Average training accuracy 0.9381\n",
      "Epoch [4]\t Average validation loss 0.0073\t Average validation accuracy 0.9578\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0076\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0078\t Accuracy 0.9478\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0081\t Accuracy 0.9429\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0083\t Accuracy 0.9408\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0082\t Accuracy 0.9417\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0082\t Accuracy 0.9422\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0082\t Accuracy 0.9420\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0082\t Accuracy 0.9414\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0083\t Accuracy 0.9414\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0083\t Accuracy 0.9415\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0083\t Accuracy 0.9405\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0083\t Average training accuracy 0.9405\n",
      "Epoch [5]\t Average validation loss 0.0071\t Average validation accuracy 0.9606\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0074\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0076\t Accuracy 0.9498\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0078\t Accuracy 0.9448\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0080\t Accuracy 0.9426\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0080\t Accuracy 0.9435\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0079\t Accuracy 0.9437\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0080\t Accuracy 0.9436\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0080\t Accuracy 0.9432\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0080\t Accuracy 0.9432\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0080\t Accuracy 0.9433\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0081\t Accuracy 0.9424\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0081\t Average training accuracy 0.9423\n",
      "Epoch [6]\t Average validation loss 0.0069\t Average validation accuracy 0.9622\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0074\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0074\t Accuracy 0.9512\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0076\t Accuracy 0.9463\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0078\t Accuracy 0.9439\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0078\t Accuracy 0.9451\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0077\t Accuracy 0.9453\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0078\t Accuracy 0.9451\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0078\t Accuracy 0.9448\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0078\t Accuracy 0.9448\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0079\t Accuracy 0.9450\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0079\t Accuracy 0.9441\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0079\t Average training accuracy 0.9440\n",
      "Epoch [7]\t Average validation loss 0.0068\t Average validation accuracy 0.9642\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0073\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0073\t Accuracy 0.9520\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0075\t Accuracy 0.9478\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0077\t Accuracy 0.9456\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0076\t Accuracy 0.9467\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0076\t Accuracy 0.9467\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0077\t Accuracy 0.9464\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0077\t Accuracy 0.9460\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0077\t Accuracy 0.9460\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0077\t Accuracy 0.9462\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0078\t Accuracy 0.9452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0077\t Average training accuracy 0.9452\n",
      "Epoch [8]\t Average validation loss 0.0067\t Average validation accuracy 0.9646\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0072\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0072\t Accuracy 0.9525\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0074\t Accuracy 0.9487\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0076\t Accuracy 0.9468\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0075\t Accuracy 0.9475\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0075\t Accuracy 0.9475\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0075\t Accuracy 0.9472\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0076\t Accuracy 0.9469\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0076\t Accuracy 0.9470\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0076\t Accuracy 0.9471\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0077\t Accuracy 0.9463\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0076\t Average training accuracy 0.9463\n",
      "Epoch [9]\t Average validation loss 0.0066\t Average validation accuracy 0.9644\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0071\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0071\t Accuracy 0.9537\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0073\t Accuracy 0.9496\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0075\t Accuracy 0.9477\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0074\t Accuracy 0.9484\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0074\t Accuracy 0.9484\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0074\t Accuracy 0.9480\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0075\t Accuracy 0.9477\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0075\t Accuracy 0.9478\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0075\t Accuracy 0.9479\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0076\t Accuracy 0.9471\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0075\t Average training accuracy 0.9471\n",
      "Epoch [10]\t Average validation loss 0.0066\t Average validation accuracy 0.9650\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0070\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0070\t Accuracy 0.9547\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0072\t Accuracy 0.9505\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0074\t Accuracy 0.9488\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0073\t Accuracy 0.9497\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0073\t Accuracy 0.9494\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0074\t Accuracy 0.9490\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0074\t Accuracy 0.9486\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0074\t Accuracy 0.9488\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0074\t Accuracy 0.9489\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0075\t Accuracy 0.9481\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0075\t Average training accuracy 0.9481\n",
      "Epoch [11]\t Average validation loss 0.0065\t Average validation accuracy 0.9654\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0070\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0069\t Accuracy 0.9559\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0071\t Accuracy 0.9515\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0073\t Accuracy 0.9499\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0073\t Accuracy 0.9505\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0073\t Accuracy 0.9503\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0073\t Accuracy 0.9497\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0073\t Accuracy 0.9495\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0074\t Accuracy 0.9497\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0074\t Accuracy 0.9497\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0074\t Accuracy 0.9489\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0074\t Average training accuracy 0.9488\n",
      "Epoch [12]\t Average validation loss 0.0064\t Average validation accuracy 0.9652\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0069\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0069\t Accuracy 0.9575\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0071\t Accuracy 0.9526\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0073\t Accuracy 0.9509\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0072\t Accuracy 0.9514\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0072\t Accuracy 0.9511\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9504\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0073\t Accuracy 0.9502\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9504\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0073\t Accuracy 0.9505\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0074\t Accuracy 0.9498\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0074\t Average training accuracy 0.9496\n",
      "Epoch [13]\t Average validation loss 0.0064\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0069\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9578\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9531\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9517\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0072\t Accuracy 0.9522\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9521\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9514\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9509\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0073\t Accuracy 0.9511\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0073\t Accuracy 0.9512\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0073\t Accuracy 0.9504\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0073\t Average training accuracy 0.9503\n",
      "Epoch [14]\t Average validation loss 0.0064\t Average validation accuracy 0.9666\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0068\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0068\t Accuracy 0.9582\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9536\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0072\t Accuracy 0.9519\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9523\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9523\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0072\t Accuracy 0.9516\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9512\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9514\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9515\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0073\t Accuracy 0.9507\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0073\t Average training accuracy 0.9506\n",
      "Epoch [15]\t Average validation loss 0.0063\t Average validation accuracy 0.9668\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0068\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0067\t Accuracy 0.9584\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0070\t Accuracy 0.9539\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9523\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9527\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0071\t Accuracy 0.9526\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9520\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0072\t Accuracy 0.9516\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0072\t Accuracy 0.9518\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9520\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9511\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0072\t Average training accuracy 0.9510\n",
      "Epoch [16]\t Average validation loss 0.0063\t Average validation accuracy 0.9672\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0067\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0067\t Accuracy 0.9582\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0069\t Accuracy 0.9542\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9528\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0071\t Accuracy 0.9532\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0070\t Accuracy 0.9531\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0071\t Accuracy 0.9521\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0071\t Accuracy 0.9523\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0072\t Accuracy 0.9525\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9517\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0072\t Average training accuracy 0.9516\n",
      "Epoch [17]\t Average validation loss 0.0063\t Average validation accuracy 0.9674\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0067\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0067\t Accuracy 0.9590\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0069\t Accuracy 0.9550\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0071\t Accuracy 0.9534\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0070\t Accuracy 0.9538\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0070\t Accuracy 0.9535\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0071\t Accuracy 0.9528\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0071\t Accuracy 0.9524\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0071\t Accuracy 0.9526\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0071\t Accuracy 0.9528\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0072\t Accuracy 0.9520\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0072\t Average training accuracy 0.9519\n",
      "Epoch [18]\t Average validation loss 0.0063\t Average validation accuracy 0.9676\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0067\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0066\t Accuracy 0.9588\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0069\t Accuracy 0.9550\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0070\t Accuracy 0.9534\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0070\t Accuracy 0.9539\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0070\t Accuracy 0.9537\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0070\t Accuracy 0.9530\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0071\t Accuracy 0.9526\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0071\t Accuracy 0.9528\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0071\t Accuracy 0.9530\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0071\t Accuracy 0.9522\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0071\t Average training accuracy 0.9520\n",
      "Epoch [19]\t Average validation loss 0.0062\t Average validation accuracy 0.9676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9531.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2833\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.2491\t Accuracy 0.1782\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2357\t Accuracy 0.2480\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.2299\t Accuracy 0.3062\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.2256\t Accuracy 0.3678\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.2222\t Accuracy 0.4203\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.2193\t Accuracy 0.4648\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.2168\t Accuracy 0.5030\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.2147\t Accuracy 0.5354\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.2127\t Accuracy 0.5618\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.2110\t Accuracy 0.5848\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2093\t Average training accuracy 0.6044\n",
      "Epoch [0]\t Average validation loss 0.1902\t Average validation accuracy 0.8442\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1904\t Accuracy 0.8400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1913\t Accuracy 0.8222\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1909\t Accuracy 0.8245\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1908\t Accuracy 0.8175\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1905\t Accuracy 0.8199\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1900\t Accuracy 0.8204\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1896\t Accuracy 0.8240\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1895\t Accuracy 0.8256\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1892\t Accuracy 0.8280\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1889\t Accuracy 0.8297\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1887\t Accuracy 0.8314\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1884\t Average training accuracy 0.8334\n",
      "Epoch [1]\t Average validation loss 0.1830\t Average validation accuracy 0.8890\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1834\t Accuracy 0.8900\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1846\t Accuracy 0.8600\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1847\t Accuracy 0.8584\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1851\t Accuracy 0.8517\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1851\t Accuracy 0.8534\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1850\t Accuracy 0.8540\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1849\t Accuracy 0.8551\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1850\t Accuracy 0.8551\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1849\t Accuracy 0.8566\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1849\t Accuracy 0.8572\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1848\t Accuracy 0.8574\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1847\t Average training accuracy 0.8581\n",
      "Epoch [2]\t Average validation loss 0.1810\t Average validation accuracy 0.9032\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1816\t Accuracy 0.9100\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1827\t Accuracy 0.8722\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1828\t Accuracy 0.8686\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1832\t Accuracy 0.8613\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1834\t Accuracy 0.8633\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1833\t Accuracy 0.8631\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1832\t Accuracy 0.8640\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1834\t Accuracy 0.8636\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1834\t Accuracy 0.8647\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1834\t Accuracy 0.8649\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1834\t Accuracy 0.8647\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1833\t Average training accuracy 0.8653\n",
      "Epoch [3]\t Average validation loss 0.1800\t Average validation accuracy 0.9070\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1808\t Accuracy 0.9100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1817\t Accuracy 0.8771\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1818\t Accuracy 0.8739\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1822\t Accuracy 0.8670\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1824\t Accuracy 0.8685\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1823\t Accuracy 0.8684\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1823\t Accuracy 0.8686\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1825\t Accuracy 0.8678\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1825\t Accuracy 0.8686\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1825\t Accuracy 0.8686\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1825\t Accuracy 0.8682\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1824\t Average training accuracy 0.8686\n",
      "Epoch [4]\t Average validation loss 0.1792\t Average validation accuracy 0.9078\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1801\t Accuracy 0.9100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1809\t Accuracy 0.8798\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1810\t Accuracy 0.8753\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1814\t Accuracy 0.8693\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1816\t Accuracy 0.8705\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1815\t Accuracy 0.8706\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1815\t Accuracy 0.8709\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1817\t Accuracy 0.8700\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1817\t Accuracy 0.8706\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1817\t Accuracy 0.8704\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1817\t Accuracy 0.8700\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1816\t Average training accuracy 0.8703\n",
      "Epoch [5]\t Average validation loss 0.1785\t Average validation accuracy 0.9112\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1794\t Accuracy 0.9300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1801\t Accuracy 0.8818\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1802\t Accuracy 0.8772\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1807\t Accuracy 0.8703\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1808\t Accuracy 0.8720\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1807\t Accuracy 0.8721\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1807\t Accuracy 0.8721\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1809\t Accuracy 0.8711\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1809\t Accuracy 0.8716\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1809\t Accuracy 0.8713\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1809\t Accuracy 0.8709\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1809\t Average training accuracy 0.8712\n",
      "Epoch [6]\t Average validation loss 0.1777\t Average validation accuracy 0.9134\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1787\t Accuracy 0.9300\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1793\t Accuracy 0.8829\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1795\t Accuracy 0.8778\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1799\t Accuracy 0.8712\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1801\t Accuracy 0.8725\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1800\t Accuracy 0.8727\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1800\t Accuracy 0.8731\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1802\t Accuracy 0.8719\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1802\t Accuracy 0.8724\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1802\t Accuracy 0.8719\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1802\t Accuracy 0.8720\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1802\t Average training accuracy 0.8722\n",
      "Epoch [7]\t Average validation loss 0.1770\t Average validation accuracy 0.9136\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1779\t Accuracy 0.9300\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1787\t Accuracy 0.8837\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1788\t Accuracy 0.8778\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1793\t Accuracy 0.8717\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1794\t Accuracy 0.8737\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1794\t Accuracy 0.8739\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1793\t Accuracy 0.8744\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1796\t Accuracy 0.8732\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1795\t Accuracy 0.8735\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1796\t Accuracy 0.8728\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1796\t Accuracy 0.8728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.1795\t Average training accuracy 0.8729\n",
      "Epoch [8]\t Average validation loss 0.1764\t Average validation accuracy 0.9160\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1772\t Accuracy 0.9300\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1781\t Accuracy 0.8847\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1782\t Accuracy 0.8788\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1787\t Accuracy 0.8728\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1788\t Accuracy 0.8743\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1788\t Accuracy 0.8746\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1787\t Accuracy 0.8753\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1790\t Accuracy 0.8738\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1790\t Accuracy 0.8742\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1790\t Accuracy 0.8736\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1790\t Accuracy 0.8736\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1790\t Average training accuracy 0.8740\n",
      "Epoch [9]\t Average validation loss 0.1758\t Average validation accuracy 0.9166\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1766\t Accuracy 0.9400\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1776\t Accuracy 0.8857\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1777\t Accuracy 0.8804\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1782\t Accuracy 0.8750\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1783\t Accuracy 0.8762\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1783\t Accuracy 0.8764\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1782\t Accuracy 0.8770\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1785\t Accuracy 0.8753\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1784\t Accuracy 0.8757\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1785\t Accuracy 0.8752\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1785\t Accuracy 0.8751\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1785\t Average training accuracy 0.8755\n",
      "Epoch [10]\t Average validation loss 0.1753\t Average validation accuracy 0.9170\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1762\t Accuracy 0.9400\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1771\t Accuracy 0.8873\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1773\t Accuracy 0.8819\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1778\t Accuracy 0.8767\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1779\t Accuracy 0.8778\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1778\t Accuracy 0.8781\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1778\t Accuracy 0.8787\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1780\t Accuracy 0.8769\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1780\t Accuracy 0.8773\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1781\t Accuracy 0.8769\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1781\t Accuracy 0.8767\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1781\t Average training accuracy 0.8771\n",
      "Epoch [11]\t Average validation loss 0.1750\t Average validation accuracy 0.9182\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1758\t Accuracy 0.9400\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1768\t Accuracy 0.8888\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1769\t Accuracy 0.8833\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1774\t Accuracy 0.8779\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1775\t Accuracy 0.8790\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1775\t Accuracy 0.8793\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1774\t Accuracy 0.8803\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1777\t Accuracy 0.8781\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1777\t Accuracy 0.8785\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1777\t Accuracy 0.8783\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1778\t Accuracy 0.8779\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1777\t Average training accuracy 0.8783\n",
      "Epoch [12]\t Average validation loss 0.1746\t Average validation accuracy 0.9184\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1755\t Accuracy 0.9400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1765\t Accuracy 0.8912\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1767\t Accuracy 0.8856\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1771\t Accuracy 0.8801\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1772\t Accuracy 0.8810\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1772\t Accuracy 0.8810\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1771\t Accuracy 0.8819\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1774\t Accuracy 0.8797\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1774\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1775\t Accuracy 0.8797\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1775\t Accuracy 0.8794\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1775\t Average training accuracy 0.8797\n",
      "Epoch [13]\t Average validation loss 0.1744\t Average validation accuracy 0.9194\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1752\t Accuracy 0.9400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1762\t Accuracy 0.8920\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1764\t Accuracy 0.8858\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1769\t Accuracy 0.8807\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1770\t Accuracy 0.8815\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1770\t Accuracy 0.8817\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1769\t Accuracy 0.8826\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1772\t Accuracy 0.8805\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1772\t Accuracy 0.8807\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1772\t Accuracy 0.8804\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1773\t Accuracy 0.8801\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1773\t Average training accuracy 0.8804\n",
      "Epoch [14]\t Average validation loss 0.1742\t Average validation accuracy 0.9188\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1751\t Accuracy 0.9400\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1760\t Accuracy 0.8918\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1762\t Accuracy 0.8858\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1767\t Accuracy 0.8809\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1768\t Accuracy 0.8820\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1768\t Accuracy 0.8821\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1767\t Accuracy 0.8831\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1770\t Accuracy 0.8811\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1770\t Accuracy 0.8811\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1771\t Accuracy 0.8809\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1771\t Accuracy 0.8807\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1771\t Average training accuracy 0.8810\n",
      "Epoch [15]\t Average validation loss 0.1740\t Average validation accuracy 0.9190\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1749\t Accuracy 0.9400\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1759\t Accuracy 0.8925\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1761\t Accuracy 0.8860\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1766\t Accuracy 0.8811\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1767\t Accuracy 0.8824\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1766\t Accuracy 0.8826\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1766\t Accuracy 0.8837\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1769\t Accuracy 0.8815\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1769\t Accuracy 0.8816\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1770\t Accuracy 0.8814\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1770\t Accuracy 0.8811\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1770\t Average training accuracy 0.8814\n",
      "Epoch [16]\t Average validation loss 0.1739\t Average validation accuracy 0.9198\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1748\t Accuracy 0.9400\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1758\t Accuracy 0.8927\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1760\t Accuracy 0.8861\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1765\t Accuracy 0.8813\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1766\t Accuracy 0.8827\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1765\t Accuracy 0.8827\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1765\t Accuracy 0.8838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1768\t Accuracy 0.8817\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1768\t Accuracy 0.8817\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1769\t Accuracy 0.8816\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1769\t Accuracy 0.8814\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1769\t Average training accuracy 0.8816\n",
      "Epoch [17]\t Average validation loss 0.1738\t Average validation accuracy 0.9208\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1747\t Accuracy 0.9400\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1757\t Accuracy 0.8925\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1759\t Accuracy 0.8860\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1764\t Accuracy 0.8816\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1765\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1765\t Accuracy 0.8829\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1764\t Accuracy 0.8840\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1767\t Accuracy 0.8820\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1767\t Accuracy 0.8820\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1768\t Accuracy 0.8820\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1768\t Accuracy 0.8818\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1768\t Average training accuracy 0.8820\n",
      "Epoch [18]\t Average validation loss 0.1738\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1746\t Accuracy 0.9400\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.1756\t Accuracy 0.8927\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.1759\t Accuracy 0.8864\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.1763\t Accuracy 0.8819\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.1764\t Accuracy 0.8833\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.1764\t Accuracy 0.8831\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.1764\t Accuracy 0.8842\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.1766\t Accuracy 0.8822\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.1767\t Accuracy 0.8824\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.1767\t Accuracy 0.8823\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.1768\t Accuracy 0.8821\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1767\t Average training accuracy 0.8823\n",
      "Epoch [19]\t Average validation loss 0.1737\t Average validation accuracy 0.9218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9024.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2557\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.2123\t Accuracy 0.4625\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2029\t Accuracy 0.5876\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.1974\t Accuracy 0.6535\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.1935\t Accuracy 0.6982\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1903\t Accuracy 0.7296\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.1879\t Accuracy 0.7532\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.1860\t Accuracy 0.7707\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.1843\t Accuracy 0.7855\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.1829\t Accuracy 0.7975\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.1818\t Accuracy 0.8070\n",
      "\n",
      "Epoch [0]\t Average training loss 0.1807\t Average training accuracy 0.8153\n",
      "Epoch [0]\t Average validation loss 0.1659\t Average validation accuracy 0.9278\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1668\t Accuracy 0.9400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1678\t Accuracy 0.9182\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1680\t Accuracy 0.9143\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1684\t Accuracy 0.9128\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1682\t Accuracy 0.9143\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1681\t Accuracy 0.9147\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1680\t Accuracy 0.9145\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1680\t Accuracy 0.9139\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1679\t Accuracy 0.9141\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1678\t Accuracy 0.9149\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1677\t Accuracy 0.9145\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1676\t Average training accuracy 0.9153\n",
      "Epoch [1]\t Average validation loss 0.1628\t Average validation accuracy 0.9462\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1631\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1648\t Accuracy 0.9347\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1651\t Accuracy 0.9290\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1656\t Accuracy 0.9270\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1655\t Accuracy 0.9288\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1654\t Accuracy 0.9288\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1655\t Accuracy 0.9284\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1656\t Accuracy 0.9279\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1655\t Accuracy 0.9277\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1655\t Accuracy 0.9279\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1656\t Accuracy 0.9271\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1655\t Average training accuracy 0.9274\n",
      "Epoch [2]\t Average validation loss 0.1618\t Average validation accuracy 0.9508\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1620\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1637\t Accuracy 0.9418\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1639\t Accuracy 0.9360\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1644\t Accuracy 0.9337\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1643\t Accuracy 0.9351\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1643\t Accuracy 0.9354\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1644\t Accuracy 0.9349\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1645\t Accuracy 0.9344\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1645\t Accuracy 0.9343\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1645\t Accuracy 0.9345\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1646\t Accuracy 0.9335\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1646\t Average training accuracy 0.9337\n",
      "Epoch [3]\t Average validation loss 0.1612\t Average validation accuracy 0.9550\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1612\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1630\t Accuracy 0.9449\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1634\t Accuracy 0.9405\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1638\t Accuracy 0.9387\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1638\t Accuracy 0.9393\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1638\t Accuracy 0.9392\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1639\t Accuracy 0.9389\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1640\t Accuracy 0.9381\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1640\t Accuracy 0.9383\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1640\t Accuracy 0.9383\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1641\t Accuracy 0.9374\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1640\t Average training accuracy 0.9374\n",
      "Epoch [4]\t Average validation loss 0.1608\t Average validation accuracy 0.9566\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1607\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1627\t Accuracy 0.9486\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1630\t Accuracy 0.9438\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1634\t Accuracy 0.9419\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1634\t Accuracy 0.9424\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1634\t Accuracy 0.9423\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1635\t Accuracy 0.9420\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1636\t Accuracy 0.9412\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1636\t Accuracy 0.9412\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1637\t Accuracy 0.9411\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1638\t Accuracy 0.9400\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1637\t Average training accuracy 0.9401\n",
      "Epoch [5]\t Average validation loss 0.1606\t Average validation accuracy 0.9584\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1604\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1624\t Accuracy 0.9506\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1627\t Accuracy 0.9456\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1632\t Accuracy 0.9438\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1631\t Accuracy 0.9447\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1631\t Accuracy 0.9444\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1632\t Accuracy 0.9438\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1633\t Accuracy 0.9430\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1633\t Accuracy 0.9431\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1634\t Accuracy 0.9432\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1635\t Accuracy 0.9421\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1634\t Average training accuracy 0.9422\n",
      "Epoch [6]\t Average validation loss 0.1605\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1602\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1622\t Accuracy 0.9522\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1625\t Accuracy 0.9472\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1629\t Accuracy 0.9458\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1629\t Accuracy 0.9465\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1629\t Accuracy 0.9462\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1630\t Accuracy 0.9456\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1631\t Accuracy 0.9451\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1631\t Accuracy 0.9452\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1632\t Accuracy 0.9453\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1633\t Accuracy 0.9443\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1632\t Average training accuracy 0.9443\n",
      "Epoch [7]\t Average validation loss 0.1603\t Average validation accuracy 0.9612\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1600\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1620\t Accuracy 0.9537\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1623\t Accuracy 0.9486\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1628\t Accuracy 0.9472\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1627\t Accuracy 0.9480\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1627\t Accuracy 0.9475\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1628\t Accuracy 0.9468\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1629\t Accuracy 0.9464\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1630\t Accuracy 0.9466\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1630\t Accuracy 0.9465\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1631\t Accuracy 0.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.1631\t Average training accuracy 0.9456\n",
      "Epoch [8]\t Average validation loss 0.1601\t Average validation accuracy 0.9630\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1599\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1618\t Accuracy 0.9551\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1622\t Accuracy 0.9495\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1626\t Accuracy 0.9483\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1626\t Accuracy 0.9491\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1626\t Accuracy 0.9486\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1627\t Accuracy 0.9478\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1628\t Accuracy 0.9474\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1628\t Accuracy 0.9476\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1629\t Accuracy 0.9477\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1630\t Accuracy 0.9467\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1629\t Average training accuracy 0.9467\n",
      "Epoch [9]\t Average validation loss 0.1601\t Average validation accuracy 0.9636\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1598\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1617\t Accuracy 0.9557\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1620\t Accuracy 0.9502\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1625\t Accuracy 0.9491\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1624\t Accuracy 0.9499\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1624\t Accuracy 0.9496\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1626\t Accuracy 0.9489\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1627\t Accuracy 0.9484\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1627\t Accuracy 0.9486\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1627\t Accuracy 0.9487\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1628\t Accuracy 0.9477\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1628\t Average training accuracy 0.9478\n",
      "Epoch [10]\t Average validation loss 0.1600\t Average validation accuracy 0.9644\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1596\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1616\t Accuracy 0.9559\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1619\t Accuracy 0.9505\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1624\t Accuracy 0.9497\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1623\t Accuracy 0.9504\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1623\t Accuracy 0.9501\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1624\t Accuracy 0.9495\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1626\t Accuracy 0.9491\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1626\t Accuracy 0.9491\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1626\t Accuracy 0.9493\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1627\t Accuracy 0.9483\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1627\t Average training accuracy 0.9484\n",
      "Epoch [11]\t Average validation loss 0.1599\t Average validation accuracy 0.9650\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1595\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1615\t Accuracy 0.9571\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1618\t Accuracy 0.9516\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1623\t Accuracy 0.9505\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1622\t Accuracy 0.9511\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1622\t Accuracy 0.9509\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1624\t Accuracy 0.9503\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1625\t Accuracy 0.9499\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1625\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1625\t Accuracy 0.9502\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1626\t Accuracy 0.9492\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1626\t Average training accuracy 0.9493\n",
      "Epoch [12]\t Average validation loss 0.1598\t Average validation accuracy 0.9650\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1594\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1615\t Accuracy 0.9578\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1618\t Accuracy 0.9523\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1622\t Accuracy 0.9511\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1621\t Accuracy 0.9517\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1622\t Accuracy 0.9516\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1623\t Accuracy 0.9510\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1624\t Accuracy 0.9505\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1624\t Accuracy 0.9506\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1625\t Accuracy 0.9508\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1626\t Accuracy 0.9498\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1625\t Average training accuracy 0.9499\n",
      "Epoch [13]\t Average validation loss 0.1598\t Average validation accuracy 0.9654\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1593\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1614\t Accuracy 0.9576\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1617\t Accuracy 0.9525\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1621\t Accuracy 0.9514\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1621\t Accuracy 0.9522\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1621\t Accuracy 0.9520\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1622\t Accuracy 0.9513\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1623\t Accuracy 0.9508\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1623\t Accuracy 0.9508\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1624\t Accuracy 0.9511\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1625\t Accuracy 0.9502\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1625\t Average training accuracy 0.9503\n",
      "Epoch [14]\t Average validation loss 0.1597\t Average validation accuracy 0.9654\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1593\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1613\t Accuracy 0.9576\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1616\t Accuracy 0.9528\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1621\t Accuracy 0.9519\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1620\t Accuracy 0.9527\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1620\t Accuracy 0.9524\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1622\t Accuracy 0.9518\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1623\t Accuracy 0.9514\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1623\t Accuracy 0.9513\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1623\t Accuracy 0.9516\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1624\t Accuracy 0.9507\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1624\t Average training accuracy 0.9508\n",
      "Epoch [15]\t Average validation loss 0.1597\t Average validation accuracy 0.9656\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1593\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1613\t Accuracy 0.9578\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1616\t Accuracy 0.9530\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1620\t Accuracy 0.9523\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1620\t Accuracy 0.9532\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1620\t Accuracy 0.9530\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1621\t Accuracy 0.9523\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1622\t Accuracy 0.9519\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1622\t Accuracy 0.9518\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1623\t Accuracy 0.9522\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1624\t Accuracy 0.9513\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1624\t Average training accuracy 0.9514\n",
      "Epoch [16]\t Average validation loss 0.1597\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1592\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1612\t Accuracy 0.9580\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1616\t Accuracy 0.9535\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1620\t Accuracy 0.9526\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1619\t Accuracy 0.9535\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1619\t Accuracy 0.9533\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1621\t Accuracy 0.9526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1622\t Accuracy 0.9522\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1622\t Accuracy 0.9522\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1622\t Accuracy 0.9525\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1624\t Accuracy 0.9517\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1623\t Average training accuracy 0.9518\n",
      "Epoch [17]\t Average validation loss 0.1596\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1591\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1612\t Accuracy 0.9586\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1615\t Accuracy 0.9537\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1619\t Accuracy 0.9528\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1619\t Accuracy 0.9537\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1619\t Accuracy 0.9536\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1620\t Accuracy 0.9529\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1621\t Accuracy 0.9525\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1622\t Accuracy 0.9526\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1622\t Accuracy 0.9529\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1623\t Accuracy 0.9521\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1623\t Average training accuracy 0.9521\n",
      "Epoch [18]\t Average validation loss 0.1596\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1591\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.1612\t Accuracy 0.9590\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.1615\t Accuracy 0.9541\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.1619\t Accuracy 0.9532\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.1619\t Accuracy 0.9542\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.1619\t Accuracy 0.9540\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.1620\t Accuracy 0.9533\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.1621\t Accuracy 0.9528\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.1621\t Accuracy 0.9529\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.1622\t Accuracy 0.9532\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.1623\t Accuracy 0.9524\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1622\t Average training accuracy 0.9524\n",
      "Epoch [19]\t Average validation loss 0.1596\t Average validation accuracy 0.9662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9534.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework-2, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of experiment report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 256))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(256, 64))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 0.2553\t Accuracy 0.1500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.2212\t Accuracy 0.3712\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.2130\t Accuracy 0.4966\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.2074\t Accuracy 0.5649\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.2029\t Accuracy 0.6161\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.1992\t Accuracy 0.6571\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.1962\t Accuracy 0.6890\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.1938\t Accuracy 0.7139\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.1916\t Accuracy 0.7338\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.1898\t Accuracy 0.7508\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.1882\t Accuracy 0.7639\n",
      "\n",
      "Epoch [0]\t Average training loss 0.1868\t Average training accuracy 0.7761\n",
      "Epoch [0]\t Average validation loss 0.1693\t Average validation accuracy 0.9280\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1699\t Accuracy 0.9300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1704\t Accuracy 0.9149\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1704\t Accuracy 0.9114\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1706\t Accuracy 0.9092\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1703\t Accuracy 0.9105\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1700\t Accuracy 0.9121\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1697\t Accuracy 0.9128\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1696\t Accuracy 0.9130\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1693\t Accuracy 0.9141\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1691\t Accuracy 0.9153\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1690\t Accuracy 0.9156\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1687\t Average training accuracy 0.9167\n",
      "Epoch [1]\t Average validation loss 0.1629\t Average validation accuracy 0.9498\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1634\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1646\t Accuracy 0.9398\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1650\t Accuracy 0.9332\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1653\t Accuracy 0.9325\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1652\t Accuracy 0.9335\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1651\t Accuracy 0.9342\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1651\t Accuracy 0.9340\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1651\t Accuracy 0.9335\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1651\t Accuracy 0.9336\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1650\t Accuracy 0.9341\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1650\t Accuracy 0.9337\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1649\t Average training accuracy 0.9340\n",
      "Epoch [2]\t Average validation loss 0.1608\t Average validation accuracy 0.9580\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1613\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1626\t Accuracy 0.9480\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1630\t Accuracy 0.9429\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1634\t Accuracy 0.9415\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1634\t Accuracy 0.9427\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1633\t Accuracy 0.9436\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1634\t Accuracy 0.9429\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1635\t Accuracy 0.9425\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1634\t Accuracy 0.9424\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1634\t Accuracy 0.9425\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1635\t Accuracy 0.9420\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1634\t Average training accuracy 0.9420\n",
      "Epoch [3]\t Average validation loss 0.1600\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1604\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1617\t Accuracy 0.9524\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1621\t Accuracy 0.9470\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1625\t Accuracy 0.9462\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1625\t Accuracy 0.9472\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1625\t Accuracy 0.9476\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1625\t Accuracy 0.9470\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1626\t Accuracy 0.9465\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1626\t Accuracy 0.9467\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1627\t Accuracy 0.9468\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1627\t Accuracy 0.9462\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1627\t Average training accuracy 0.9462\n",
      "Epoch [4]\t Average validation loss 0.1595\t Average validation accuracy 0.9640\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1598\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1611\t Accuracy 0.9543\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1616\t Accuracy 0.9501\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1620\t Accuracy 0.9491\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1619\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1619\t Accuracy 0.9504\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1620\t Accuracy 0.9496\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1621\t Accuracy 0.9493\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1621\t Accuracy 0.9496\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1621\t Accuracy 0.9498\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1622\t Accuracy 0.9490\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1622\t Average training accuracy 0.9492\n",
      "Epoch [5]\t Average validation loss 0.1591\t Average validation accuracy 0.9652\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1594\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1608\t Accuracy 0.9575\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1612\t Accuracy 0.9537\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1616\t Accuracy 0.9523\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1616\t Accuracy 0.9530\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1616\t Accuracy 0.9531\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1616\t Accuracy 0.9525\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1617\t Accuracy 0.9521\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1617\t Accuracy 0.9524\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1618\t Accuracy 0.9524\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1619\t Accuracy 0.9516\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1618\t Average training accuracy 0.9517\n",
      "Epoch [6]\t Average validation loss 0.1588\t Average validation accuracy 0.9662\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1591\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1605\t Accuracy 0.9588\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1609\t Accuracy 0.9552\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1613\t Accuracy 0.9536\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1613\t Accuracy 0.9544\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1613\t Accuracy 0.9545\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1614\t Accuracy 0.9538\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1615\t Accuracy 0.9536\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1615\t Accuracy 0.9539\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1615\t Accuracy 0.9539\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1616\t Accuracy 0.9531\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1615\t Average training accuracy 0.9531\n",
      "Epoch [7]\t Average validation loss 0.1586\t Average validation accuracy 0.9670\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1589\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1603\t Accuracy 0.9606\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1607\t Accuracy 0.9573\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1611\t Accuracy 0.9555\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1610\t Accuracy 0.9560\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1610\t Accuracy 0.9559\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1611\t Accuracy 0.9551\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1612\t Accuracy 0.9550\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1612\t Accuracy 0.9553\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1613\t Accuracy 0.9553\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1614\t Accuracy 0.9546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.1613\t Average training accuracy 0.9546\n",
      "Epoch [8]\t Average validation loss 0.1584\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.1587\t Accuracy 0.9600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1601\t Accuracy 0.9624\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1605\t Accuracy 0.9582\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1609\t Accuracy 0.9565\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1609\t Accuracy 0.9569\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1609\t Accuracy 0.9567\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1610\t Accuracy 0.9558\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1611\t Accuracy 0.9559\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1611\t Accuracy 0.9562\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1611\t Accuracy 0.9561\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1612\t Accuracy 0.9555\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1611\t Average training accuracy 0.9555\n",
      "Epoch [9]\t Average validation loss 0.1583\t Average validation accuracy 0.9688\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.1585\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.1599\t Accuracy 0.9629\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1604\t Accuracy 0.9590\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1607\t Accuracy 0.9573\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1607\t Accuracy 0.9577\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1607\t Accuracy 0.9574\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1608\t Accuracy 0.9566\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1609\t Accuracy 0.9566\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1609\t Accuracy 0.9569\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1610\t Accuracy 0.9568\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1610\t Accuracy 0.9561\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1610\t Average training accuracy 0.9561\n",
      "Epoch [10]\t Average validation loss 0.1582\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.1584\t Accuracy 0.9600\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.1598\t Accuracy 0.9627\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.1603\t Accuracy 0.9588\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1606\t Accuracy 0.9575\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1606\t Accuracy 0.9580\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1606\t Accuracy 0.9576\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1607\t Accuracy 0.9569\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1608\t Accuracy 0.9570\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1608\t Accuracy 0.9573\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1608\t Accuracy 0.9572\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1609\t Accuracy 0.9566\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1609\t Average training accuracy 0.9566\n",
      "Epoch [11]\t Average validation loss 0.1581\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.1583\t Accuracy 0.9600\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.1597\t Accuracy 0.9635\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.1601\t Accuracy 0.9594\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.1605\t Accuracy 0.9581\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.1605\t Accuracy 0.9585\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.1605\t Accuracy 0.9581\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.1606\t Accuracy 0.9573\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.1607\t Accuracy 0.9574\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1607\t Accuracy 0.9577\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1607\t Accuracy 0.9576\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1608\t Accuracy 0.9570\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1608\t Average training accuracy 0.9571\n",
      "Epoch [12]\t Average validation loss 0.1581\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.1582\t Accuracy 0.9600\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.1596\t Accuracy 0.9637\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.1601\t Accuracy 0.9599\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.1604\t Accuracy 0.9587\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.1604\t Accuracy 0.9591\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.1604\t Accuracy 0.9588\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.1605\t Accuracy 0.9580\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.1606\t Accuracy 0.9580\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.1606\t Accuracy 0.9582\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.1606\t Accuracy 0.9582\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.1607\t Accuracy 0.9575\n",
      "\n",
      "Epoch [13]\t Average training loss 0.1607\t Average training accuracy 0.9576\n",
      "Epoch [13]\t Average validation loss 0.1580\t Average validation accuracy 0.9690\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.1582\t Accuracy 0.9600\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.1595\t Accuracy 0.9637\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.1600\t Accuracy 0.9603\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.1603\t Accuracy 0.9588\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.1603\t Accuracy 0.9592\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.1603\t Accuracy 0.9589\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.1604\t Accuracy 0.9582\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.1605\t Accuracy 0.9583\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.1605\t Accuracy 0.9584\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.1605\t Accuracy 0.9583\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.1606\t Accuracy 0.9577\n",
      "\n",
      "Epoch [14]\t Average training loss 0.1606\t Average training accuracy 0.9578\n",
      "Epoch [14]\t Average validation loss 0.1579\t Average validation accuracy 0.9692\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.1581\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.1594\t Accuracy 0.9645\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.1599\t Accuracy 0.9607\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.1603\t Accuracy 0.9591\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.1602\t Accuracy 0.9597\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.1602\t Accuracy 0.9594\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.1603\t Accuracy 0.9587\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.1604\t Accuracy 0.9588\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.1604\t Accuracy 0.9589\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.1605\t Accuracy 0.9587\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.1606\t Accuracy 0.9581\n",
      "\n",
      "Epoch [15]\t Average training loss 0.1605\t Average training accuracy 0.9582\n",
      "Epoch [15]\t Average validation loss 0.1579\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.1580\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.1594\t Accuracy 0.9645\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.1598\t Accuracy 0.9609\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.1602\t Accuracy 0.9595\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.1602\t Accuracy 0.9600\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.1602\t Accuracy 0.9597\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.1603\t Accuracy 0.9590\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.1603\t Accuracy 0.9590\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.1604\t Accuracy 0.9591\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.1604\t Accuracy 0.9590\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.1605\t Accuracy 0.9583\n",
      "\n",
      "Epoch [16]\t Average training loss 0.1605\t Average training accuracy 0.9584\n",
      "Epoch [16]\t Average validation loss 0.1578\t Average validation accuracy 0.9700\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.1580\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.1593\t Accuracy 0.9645\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.1598\t Accuracy 0.9609\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.1601\t Accuracy 0.9596\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.1601\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.1601\t Accuracy 0.9598\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.1602\t Accuracy 0.9590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.1603\t Accuracy 0.9591\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.1603\t Accuracy 0.9592\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.1603\t Accuracy 0.9591\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.1604\t Accuracy 0.9585\n",
      "\n",
      "Epoch [17]\t Average training loss 0.1604\t Average training accuracy 0.9586\n",
      "Epoch [17]\t Average validation loss 0.1578\t Average validation accuracy 0.9708\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.1579\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.1593\t Accuracy 0.9647\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.1597\t Accuracy 0.9611\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.1601\t Accuracy 0.9599\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.1600\t Accuracy 0.9602\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.1600\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.1601\t Accuracy 0.9592\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.1602\t Accuracy 0.9593\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.1602\t Accuracy 0.9594\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.1603\t Accuracy 0.9593\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.1604\t Accuracy 0.9587\n",
      "\n",
      "Epoch [18]\t Average training loss 0.1603\t Average training accuracy 0.9588\n",
      "Epoch [18]\t Average validation loss 0.1578\t Average validation accuracy 0.9710\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.1579\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.1592\t Accuracy 0.9647\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.1597\t Accuracy 0.9613\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.1600\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.1600\t Accuracy 0.9603\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.1600\t Accuracy 0.9602\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.1601\t Accuracy 0.9594\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.1602\t Accuracy 0.9596\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.1602\t Accuracy 0.9597\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.1602\t Accuracy 0.9595\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.1603\t Accuracy 0.9589\n",
      "\n",
      "Epoch [19]\t Average training loss 0.1603\t Average training accuracy 0.9590\n",
      "Epoch [19]\t Average validation loss 0.1577\t Average validation accuracy 0.9710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9582.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3X+cVmWd//HXm4FhgAFFZiRlENBQwdb1x4SotRCuhq5J6rZKtWrrZtmarZvt6mpZpKmt39pttTYyf9ZqrqVRaYqktrVqDBkqIjihyYg/BvEHiDgM8/n+cc7o7TAz3Afm3PfN8H4+HvfjPuc61znncw831+c+13V+KCIwMzMr1oByB2BmZtsXJw4zM8vEicPMzDJx4jAzs0ycOMzMLBMnDjMzyyS3xCHpGkkvSnqsh+WS9C1JzZIekXRQwbJTJT2Zvk7NK0YzM8suzyOO64CZvSw/GpiYvs4AvgMgaRfgIuAQYApwkaSROcZpZmYZ5JY4IuLXwJpeqswCbojEg8DOknYDPgjMj4g1EfEyMJ/eE5CZmZXQwDLuewywsmC+JS3rqXwzks4gOVph2LBhB++77775RGpm1k8tWrRodUTUZ1mnnIlD3ZRFL+WbF0bMBeYCNDY2RlNTU99FZ2a2A5D0p6zrlPOsqhZgbMF8A7Cql3IzM6sA5Uwc84BT0rOrpgKvRsRzwF3AUZJGpoPiR6VlZmZWAXLrqpJ0EzAdqJPUQnKm1CCAiPgv4A7gGKAZWA98Il22RtJXgYXppuZERG+D7GZmVkK5JY6ImL2F5QH8Qw/LrgGuySMuM7PebNy4kZaWFjZs2FDuUPpUTU0NDQ0NDBo0aJu3Vc7BcTOzitPS0sLw4cMZP348Unfn6mx/IoKXXnqJlpYWJkyYsM3b8y1HzMwKbNiwgVGjRvWbpAEgiVGjRvXZUZQTh5lZF/0paXTqy8/kxGFmZpk4cZiZbaemT59OOS589uC4mdlWarx4PqvXtW1WXldbTdOFR/bJPiKCiGDAgMr5nV85kZiZbWe6Sxq9lRfr6aefZtKkSXzmM5/hoIMO4sYbb+TQQw/loIMO4iMf+Qjr1q3bbJ3a2tq3pm+99VZOO+20bYqhNz7iMDPrwVd+toTHV722Veue9N0Hui2fvPsILvrQfltcf9myZVx77bXMmTOHE044gXvuuYdhw4Zx+eWX841vfIMvfelLWxVXX3DiMDOrQOPGjWPq1Kn8/Oc/5/HHH+fwww8HoK2tjUMPPbSssTlxmJn1YEtHBuPP+0WPy370qW1r3IcNGwYkYxxHHnkkN910U6/1C0+3zfuqd49xmJlVsKlTp/Lb3/6W5uZmANavX8/y5cs3qzd69GiWLl1KR0cHt912W64xOXGYmW2lutrqTOVbo76+nuuuu47Zs2ez//77M3XqVJ544onN6l122WUce+yxzJgxg912263P9t8dJfca3P75QU5m1heWLl3KpEmTyh1GLrr7bJIWRURjlu34iMPMzDJx4jAzs0ycOMzMuugvXfiF+vIz5Zo4JM2UtExSs6Tzulk+TtICSY9Iuk9SQ8Gyr0taImmppG+pP96u0swqTk1NDS+99FK/Sh6dz+Ooqanpk+3l+ejYKuAq4EigBVgoaV5EPF5Q7Qrghoi4XtIM4FLgbyUdBhwO7J/W+w0wDbgvr3jNzAAaGhpoaWmhtbW13KH0qc4nAPaFPC8AnAI0R8QKAEk3A7OAwsQxGTgnnb4XuD2dDqAGqAZE8qzyF3KM1cwMgEGDBvXJU/L6szy7qsYAKwvmW9KyQouBE9Pp44HhkkZFxAMkieS59HVXRCzNMVYzMytSnomjuzGJrp2G5wLTJD1M0hX1LNAu6d3AJKCBJNnMkPQXm+1AOkNSk6Sm/nZYaWZWqfJMHC3A2IL5BmBVYYWIWBURJ0TEgcAFadmrJEcfD0bEuohYB9wJTO26g4iYGxGNEdFYX1+f1+cwM7MCeSaOhcBESRMkVQMnA/MKK0iqk9QZw/nANen0MyRHIgMlDSI5GnFXlZlZBcgtcUREO3AWcBdJo39LRCyRNEfScWm16cAyScuB0cAlafmtwB+BR0nGQRZHxM/yitXMzIrne1WZme3AfK8qMzPLnROHmZll4sRhZmaZ+NGxZmYl1HjxfFava9usvK62mqYLjyzJNgrXr37Xuw8uaqcFnDjMzIrUF41+d+v3Vp7HNrLsqztOHGa2XejLX9lbsz5ka7Ajgjc2bmLthnZee2Mjr21o57UNG3vd/sU/f5z2jmBTR6TvHWzqgE0dHe8o783Hr36oqM+yLZw4zHJSaV0S5Yqhr7axtb+yOxvw3ta/f3lr0jhvertx7oh4x/ymjo5e9/PR7z3Iaxs2vpUo1m5o32Ij39VNv3uGqgFiYNUABkgMHKB0Pn0fIKoG9D40/cbGTZn2uTWcOMy6kXdDt3FTB5ve8csyaO9Iygobq9628WjLq9sUQzHrl2Ibv17e+o6/wzv+FkX+yv7UjU2sb9vE62+2s75tU8GrnTc2bmJLl6udes3vivocvWlr76C+djB71dcyvGYgI2oGMbxmECOGDEzea5L3E7/zfz1uY8mcmUXta/x5v+hx2Y/PPGyb1i+GE4f1O6Vo9F9e38Yr6zey5vU2Xlnfxstdpl9+vfdfwRMvuLOoOHrzoSt/U9b1+2obp/RBo/306vUMHVzFsOqBjKodzLDqKoZUD2RYdRVDBw9kaHUVl935RI/r//jMw976df/2L3sxcMAAqqrenm+8+J4et3FrEQ12f+HEYRVnWxv+LXVpRATr29K+5w0bWbthI6+9kUy/tqGdtVvoh+6t0R8yqIqRQwcxclh1r9s496i9qRow4B2N1TsarKqkS+Lsmx7ucRtXn7Lli33//oae76ZQzPql2MaPzzw07ZYZ0E23zNvl772k50b7rnM2u3n2ZnpLHAePG7nF9ftKXW11j9/vUm2jp/WL5cRhfaoUfdlt7R2s7exL7tKnvKXBxwPm3M3aDe1sytj3XOifjtybkcOqkwQxtDp5DUumawZVvVWvt+6As2ZMLGpfvSWOv5w8uvigc1i/r7Zx8LhdtnkbpdIXjX6x/w/y3Ebh+rr82EVZ13fisD5V7ABmRLDuzXZeWb+Rl9e3pd08yXRv9v3inWzY2PsgZW8+tP/uBX3Og5K+6CGD3uqTHpHO7/vFX/a4jbOPKK7Rt76V16/sUjf6/YETh71lW44W2jd1sGYL/fonffcBXk7HAF5Z38bGTdl/9Z9y6HiGDx5Y0OC/PejYWfbnX7m7x/W/+uH3ZN7n1qrkLolSxtBX2+jLX9m2bZw47C29HS3ct+xFVq9rY/W6N2ld+yar16WvtUnZmvVtWzxzJQIm1A3joKHV7Dy0ml2GDUre066ezukDvzq/x2386zGTtuUjFq0SGrq+2EYlxNBX27DK4cSxg+voCFa9+gZPvrCu13qnXbvwrekhg6qoG15NXe1gxo0aysHjR1JXO5j62mq++NMlPW7jlk8f2mdx96ZSGn2z/sqJo5/YUjdTR0fw7Ctv8OSLa3nyhXUsf2EdzS+u5ckX17G+bcsXDP34zEOpqx1MXe1ghg3u+WvTW+Io1rY2/G70zfLlxNFP9NbNdNyVv6G5S4LYdfhgJo6u5W8axzJxdC17jx7OR/7rgR63X+yZL/61b9b/5Zo4JM0E/gOoAq6OiMu6LB9H8pzxemAN8PGIaEmX7QFcDYwFAjgmIp7OM97t1fq29l6Xj6gZxEnvHcvEXYez9+haJu46nJ2GDsolFjf6Zv1fbolDUhVwFXAk0AIslDQvIh4vqHYFcENEXC9pBnAp8LfpshuASyJivqRaYOvPwexnIoKnVr/OvctauW/Zizz01Jpe6//g7w8part9cbRgZv1fnkccU4DmiFgBIOlmYBZQmDgmA+ek0/cCt6d1JwMDI2I+QET0PnLbD2xpjOKNtk08uOIl7lv2Ivcua+WZNesB2Kt+GKdMHcfVv3lqm2Pw0YKZFSPPxDEGWFkw3wJ0/em7GDiRpDvreGC4pFHA3sArkn4CTADuAc6LiHeM4ko6AzgDYI899sjjM5RMb2MUp17zOx5c8RJvtndQM2gAh+9VxyffP4Hp++zK2F2GAvRJ4jAzK0aeiUPdlHU90/9c4EpJpwG/Bp4F2tO43g8cCDwD/Ag4Dfj+OzYWMReYC9DY2Lj195CocCvXrOejh+zBB/bZlSkTdnnHbS06uZvJzEolz8TRQjKw3akBWFVYISJWAScApOMYJ0bEq5JagIcLurluB6bSJXHsKH517vQt1nE3k5mVSu9PBNk2C4GJkiZIqgZOBuYVVpBUJ6kzhvNJzrDqXHekpPp0fgbvHBvpN9raO/j2fc3lDsPMrGi5HXFERLuks4C7SE7HvSYilkiaAzRFxDxgOnCppCDpqvqHdN1Nks4FFkgSsAj4Xl6xlsvCp9dwwW2PsnwLV22bmVWSXK/jiIg7gDu6lH2pYPpW4NYe1p0P7J9nfOXy8uttXHbnE/yoaSVjdh7C1ac0ct5PHvEYhZltF3zleAlFBLcuauFrdyzltQ3tfGrannzuiIkMrR5I02SPUZjZ9sGJo0SaX1zLBbc9xkNPreHgcSO55Pj3sO+7RpQ7LDOzzJw4crZh4yau/FUz3/31HxlaPZBLT/gzTmocy4AB3Z2tbGZW+Zw4cnT/8la+ePtjPLNmPSccOIZ//atJ1NUOLndYZmbbxImjD/R0uxCAPeuG8d+fPITD9qorcVRmZvlw4ugDPSUNgDv/8f0MHrj5ld5mZturPC8ANHDSMLN+x4nDzMwyceIwM7NMnDjMzCwTJ45t9GjLqz0u8+1CzKw/8llV26CjI7jwp49RVzuYX507jRE1+TzH28yskviIYxvcvHAli1e+woV/NclJw8x2GE4cW+mldW9y+S+fYOqeuzDrgN3LHY6ZWck4cWyly3/5BK+/2c5XZ72H5JEhZmY7BieOrdD09BpuaWrh9PdPYOLo4eUOx8yspHJNHJJmSlomqVnSed0sHydpgaRHJN0nqaHL8hGSnpV0ZZ5xZtG+qYMLb3+M3Xeq4ewZE8sdjplZyeWWOCRVAVcBRwOTgdmSJnepdgVwQ0TsD8wBLu2y/KvA/XnFuDWuf+BPPPH8Wr70ockMG+yT0sxsx5PnEccUoDkiVkREG3AzMKtLncnAgnT63sLlkg4GRgN35xhjJi+8toFvzl/O9H3q+eB+7yp3OGZmZZFn4hgDrCyYb0nLCi0GTkynjweGSxolaQDw/4Av9LYDSWdIapLU1Nra2kdh9+ziXyylbVMHXzluPw+Im9kOK8/E0V3LGl3mzwWmSXoYmAY8C7QDnwHuiIiV9CIi5kZEY0Q01tfX90XMPfpt82p+tngVn5m+F+NGDct1X2ZmlSzPTvoWYGzBfAOwqrBCRKwCTgCQVAucGBGvSjoUeL+kzwC1QLWkdRGx2QB7KbzZvokv/vQxxo0ayqen7VWOEMzMKkaeiWMhMFHSBJIjiZOBjxZWkFQHrImIDuB84BqAiPhYQZ3TgMZyJQ2Aq//3KVa0vs61n3gvNYP8fA0z27Hl1lUVEe3AWcBdwFLglohYImmOpOPSatOBZZKWkwyEX5JXPFtr5Zr1/OevnmTmfu/iA/vsWu5wzMzKThFdhx22T42NjdHU1NTn2/3kDU38tnk19/zTNHbfeUifb9/MrJwkLYqIxizr+MrxXixY+gLzH3+Bs4+Y6KRhZpZy4ujBG22buGjeEibuWsvfHT6h3OGYmVUMX/rcg2/f10zLy29w0yenUj3Q+dXMrJNbxG6saF3Hd+9fwfEHjuHQvUaVOxwzs4rixNFFRHDRvCUMHjiA84/Zt9zhmJlVHCeOLu549Hn+98nVnPvBfdh1eE25wzEzqzg7/BhH48XzWb2ubbPy//zVk5x62PjSB2RmVuG2eMQh6SxJI0sRTDl0lzR6Kzcz29EV01X1LmChpFvSBzP5trBmZjuwLSaOiLgQmAh8HzgNeFLS1yT5bn9mZjugogbHI7kvyfPpqx0YCdwq6es5xmZmZhVoi4Pjks4GTgVWA1cDX4iIjenDlp4E/jnfEM3MrJIUc1ZVHXBCRPypsDAiOiQdm09YpVNXW93tQHhdbXUZojEzq3zFJI47gDWdM5KGA5Mj4qGIWJpbZCXSdOGR5Q7BzGy7UswYx3eAdQXzr6dlZma2AyomcSgKHtqRPq1vh79w0MxsR1VM4lgh6WxJg9LX54AVxWw8ve5jmaRmSZs9+lXSOEkLJD0i6T5JDWn5AZIekLQkXXZSto9lZmZ5KSZxfBo4jOS54S3AIcAZW1pJUhVwFXA0MBmYLWlyl2pXADdExP7AHODStHw9cEpE7AfMBP5d0s5FxGpmZjnbYpdTRLwInLwV254CNEfECgBJNwOzgMcL6kwGzkmn7wVuT/e5vGD/qyS9CNQDr2xFHGZm1oeKuY6jBjgd2A9463axEfF3W1h1DLCyYL7zaKXQYuBE4D+A44HhkkZFxEsF+58CVAN/7Ca2M0iPfvbYY48tfRQzM+sDxXRV3Uhyv6oPAvcDDcDaItbr7p5W0WX+XGCapIeBaSTdYe1vbUDaLd3/J9JB+XduLGJuRDRGRGN9fX0RIZmZ2bYq5uyod0fERyTNiojrJf03cFcR67UAYwvmG4BVhRUiYhVwAoCkWuDEiHg1nR8B/AK4MCIeLGJ/ZmZWAsUccWxM31+R9B5gJ2B8EestBCZKmiCpmmScZF5hBUl16a1LAM4HrknLq4HbSAbO/6eIfZmZWYkUkzjmps/juJCk4X8cuHxLK0VEO3AWydHJUuCWiFgiaY6k49Jq04FlkpYDo4FL0vK/Af4COE3SH9LXARk+l5mZ5UQF1/ZtvjA5GvjriLildCFtncbGxmhqaip3GGZm2xVJiyKiMcs6vR5xpAPSZ21TVGZm1q8U01U1X9K5ksZK2qXzlXtkZmZWkYo5q6rzeo1/KCgLYM++D8fMzCpdMVeOTyhFIGZmtn0o5srxU7orj4gb+j4cMzOrdMV0Vb23YLoGOAL4PeDEYWa2Ayqmq+qzhfOSdiK5DYiZme2Aijmrqqv1wMS+DsTMzLYPxYxx/Iy3b044gORW6BV/QaCZmeWjmDGOKwqm24E/RURLTvGYmVmFKyZxPAM8FxEbACQNkTQ+Ip7ONTIzM6tIxYxx/A9Q+CyMTWmZmZntgIpJHAMjoq1zJp2uzi8kMzOrZMUkjtaC26AjaRawOr+QzMyskhUzxvFp4IeSrkznW4BuryY3M7P+r5gLAP8ITE0f7aqIKOZ542Zm1k9tsatK0tck7RwR6yJiraSRki4uRXBmZlZ5ihnjODoiXumciYiXgWOK2bikmZKWSWqWdF43y8dJWiDpEUn3SWooWHaqpCfT16nF7M/MzPJXTOKokjS4c0bSEGBwL/U761UBVwFHk1xtPlvS5C7VrgBuiIj9gTnApem6uwAXAYcAU4CL0ueem5lZmRWTOH4ALJB0uqTTgfnA9UWsNwVojogV6Sm8NwOzutSZDCxIp+8tWP5BYH5ErEmPcOYDM4vYp5mZ5WyLiSMivg5cDEwiaeh/CYwrYttjgJUF8y1pWaHFwInp9PHAcEmjilwXSWdIapLU1NraWkRIZma2rYq9O+7zJFePn0jyPI6lRayjbsqiy/y5wDRJDwPTgGdJ7odVzLpExNyIaIyIxvr6+iJCMjOzbdXj6biS9gZOBmYDLwE/Ijkd9wNFbrsFGFsw3wCsKqwQEauAE9L91QInRsSrklqA6V3Wva/I/ZqZWY56O+J4guTo4kMR8b6I+E+S+1QVayEwUdIESdUkSWheYQVJdZI6YzgfuCadvgs4Kj31dyRwVFpmZmZl1lviOJGki+peSd+TdATddyF1KyLagbNIGvylwC0RsUTSnIJbmEwHlklaDowGLknXXQN8lST5LATmpGVmZlZmiths6OCdFaRhwIdJuqxmkJxRdVtE3J1/eMVrbGyMpqamcodhZrZdkbQoIhqzrFPMWVWvR8QPI+JYkrGGPwCbXcxnZmY7hkzPHE+vq/huRMzIKyAzM6tsmRKHmZmZE4eZmWXixGFmZpk4cZiZWSZOHGZmlokTh5mZZeLEYWZmmThxmJlZJk4cZmaWiROHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWWSa+KQNFPSMknNkjZ7hoekPSTdK+lhSY9IOiYtHyTpekmPSloq6fw84zQzs+LlljgkVQFXAUcDk4HZkiZ3qXYhySNlDyR5Jvm30/KPAIMj4s+Ag4FPSRqfV6xmZla8PI84pgDNEbEiItqAm4FZXeoEMCKd3glYVVA+TNJAYAjQBryWY6xmZlakPBPHGGBlwXxLWlboy8DHJbUAdwCfTctvBV4HngOeAa6IiDVddyDpDElNkppaW1v7OHwzM+tOnolD3ZRFl/nZwHUR0QAcA9woaQDJ0comYHdgAvB5SXtutrGIuRHRGBGN9fX1fRu9mZl1K8/E0QKMLZhv4O2uqE6nA7cARMQDQA1QB3wU+GVEbIyIF4HfAo05xmpmZkXKM3EsBCZKmiCpmmTwe16XOs8ARwBImkSSOFrT8hlKDAOmAk/kGKuZmRUpt8QREe3AWcBdwFKSs6eWSJoj6bi02ueBT0paDNwEnBYRQXI2Vi3wGEkCujYiHskrVjMzK56Sdnr719jYGE1NTeUOw8xsuyJpUURkGgrwleNmZpaJE4eZmWXixGFmZpk4cZiZWSZOHGZmlokTh5mZZeLEYWZmmThxmJlZJk4cZmaWiROHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWXixGFmZpk4cZiZWSa5Jg5JMyUtk9Qs6bxulu8h6V5JD0t6RNIxBcv2l/SApCWSHpVUk2esZmZWnIF5bVhSFckjYI8EWoCFkuZFxOMF1S4keaTsdyRNBu4AxksaCPwA+NuIWCxpFLAxr1jNzKx4eR5xTAGaI2JFRLQBNwOzutQJYEQ6vROwKp0+CngkIhYDRMRLEbEpx1jNzKxIeSaOMcDKgvmWtKzQl4GPS2ohOdr4bFq+NxCS7pL0e0n/3N0OJJ0hqUlSU2tra99Gb2Zm3cozcaibsugyPxu4LiIagGOAGyUNIOlCex/wsfT9eElHbLaxiLkR0RgRjfX19X0bvZmZdSvPxNECjC2Yb+DtrqhOpwO3AETEA0ANUJeue39ErI6I9SRHIwflGKuZmRUpz8SxEJgoaYKkauBkYF6XOs8ARwBImkSSOFqBu4D9JQ1NB8qnAY9jZmZll9tZVRHRLukskiRQBVwTEUskzQGaImIe8Hnge5LOIenGOi0iAnhZ0jdIkk8Ad0TEL/KK1czMiqeknd7+NTY2RlNTU7nDMDPbrkhaFBGNWdbxleNmZpaJE4eZmWXixGFmZpk4cZiZWSZOHGZmlokTh5mZZeLEYWZmmThxmJlZJk4cZmaWiROHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWXixGFmZpk4cZiZWSa5Jg5JMyUtk9Qs6bxulu8h6V5JD0t6RNIx3SxfJ+ncPOM0M7Pi5ZY4JFUBVwFHA5OB2ZImd6l2IXBLRBxI8kzyb3dZ/k3gzrxiNDOz7PI84pgCNEfEiohoA24GZnWpE8CIdHonYFXnAkkfBlYAS3KM0czMMsozcYwBVhbMt6Rlhb4MfFxSC3AH8FkAScOAfwG+kmN8Zma2FfJMHOqmLLrMzwaui4gG4BjgRkkDSBLGNyNiXa87kM6Q1CSpqbW1tU+CNjOz3g3McdstwNiC+QYKuqJSpwMzASLiAUk1QB1wCPDXkr4O7Ax0SNoQEVcWrhwRc4G5AI2NjV2TkpmZ5SDPxLEQmChpAvAsyeD3R7vUeQY4ArhO0iSgBmiNiPd3VpD0ZWBd16RhZmblkVtXVUS0A2cBdwFLSc6eWiJpjqTj0mqfBz4paTFwE3BaRPjIwcysgqm/tNONjY3R1NRU7jDMzLYrkhZFRGOWdXzluJmZZeLEYWZmmThxmJlZJk4cZmaWiROHmZll4sRhZmaZOHGYmVkmThxmZpaJE4eZmWXixGFmZpn0m1uOSFoLLCt3HCR3913tGIDKiKMSYoDKiKMSYoDKiKMSYoDKiGOfiBieZYU8745basuy3m8lD5Kayh1HJcRQKXFUQgyVEkclxFApcVRCDJUSh6TMN/lzV5WZmWXixGFmZpn0p8Qxt9wBpCohjkqIASojjkqIASojjkqIASojjkqIASojjswx9JvBcTMzK43+dMRhZmYl4MRhZmaZ9IvEIWmmpGWSmiWdV4b9j5V0r6SlkpZI+lypY+gST5WkhyX9vEz731nSrZKeSP8mh5YpjnPSf4/HJN0kqaZE+71G0ouSHiso20XSfElPpu8jyxDDv6X/Jo9Iuk3SznnG0FMcBcvOlRSS6soRg6TPpu3GEklfzzOGnuKQdICkByX9QVKTpCk5x9BtW5X5+xkR2/ULqAL+COwJVAOLgckljmE34KB0ejiwvNQxdInnn4D/Bn5epv1fD/x9Ol0N7FyGGMYATwFD0vlbgNNKtO+/AA4CHiso+zpwXjp9HnB5GWI4ChiYTl+edww9xZGWjwXuAv4E1JXhb/EB4B5gcDq/a5m+F3cDR6fTxwD35RxDt21V1u9nfzjimAI0R8SKiGgDbgZmlTKAiHguIn6fTq8FlpI0XCUnqQH4K+DqMu1/BMl/kO8DRERbRLxSjlhILnAdImkgMBRYVYqdRsSvgTVdimeRJFTS9w+XOoaIuDsi2tPZB4GGPGPoKY7UN4F/BnI/O6eHGM4ELouIN9M6L5YpjgBGpNM7kfN3tJe2KtP3sz8kjjHAyoL5FsrUaANIGg8cCDxUphD+neQ/ZEeZ9r8n0Apcm3aXXS1pWKmDiIhngSuAZ4DngFcj4u5Sx1FgdEQ8l8b2HLBrGWMB+DvgznLsWNJxwLMRsbgc+0/tDbxf0kOS7pf03jLF8Y/Av0laSfJ9Pb9UO+7SVmX6fvaHxKFuyspyjrGkWuDHwD9GxGtl2P+xwIsRsajU+y4wkORw/DsRcSDwOsmhb0mlfbSzgAnA7sAwSR8vdRyVSNIFQDvwwzLseyhwAfClUu+7i4HASGAq8AXgFkkG7PTZAAADa0lEQVTdtSV5OxM4JyLGAueQHqnnbVvbqv6QOFpI+ks7NVCiLolCkgaR/EP8MCJ+Uur9pw4HjpP0NEmX3QxJPyhxDC1AS0R0HnHdSpJISu0vgaciojUiNgI/AQ4rQxydXpC0G0D6nnvXSHcknQocC3ws0g7tEtuLJJkvTr+nDcDvJb2rxHG0AD+JxO9IjtBzHaTvwakk302A/yHpes9VD21Vpu9nf0gcC4GJkiZIqgZOBuaVMoD0l8r3gaUR8Y1S7rtQRJwfEQ0RMZ7k7/CriCjpr+yIeB5YKWmftOgI4PFSxpB6BpgqaWj673MESX9uucwjaSRI339a6gAkzQT+BTguItaXev8AEfFoROwaEePT72kLyWDt8yUO5XZgBoCkvUlO4ijHXWpXAdPS6RnAk3nurJe2Ktv3M+8zCUrxIjkbYTnJ2VUXlGH/7yPpHnsE+EP6OqbMf5PplO+sqgOApvTvcTswskxxfAV4AngMuJH0DJoS7PcmknGVjSQN4+nAKGABScOwANilDDE0k4wHdn5H/6scf4suy58m/7OquvtbVAM/SL8bvwdmlOl78T5gEcnZoA8BB+ccQ7dtVdbvp285YmZmmfSHriozMyshJw4zM8vEicPMzDJx4jAzs0ycOMzMLBMnDrMMJG1K72Ta+eqzq+Ilje/uLrJmlWZguQMw2868EREHlDsIs3LyEYdZH5D0tKTLJf0ufb07LR8naUH6DIwFkvZIy0enz8RYnL46b4dSJel76bMS7pY0pGwfyqwHThxm2Qzp0lV1UsGy1yJiCnAlyV2KSadviIj9SW4q+K20/FvA/RHx5yT38lqSlk8EroqI/YBXgBNz/jxmmfnKcbMMJK2LiNpuyp8muW3FivQmcs9HxChJq4HdImJjWv5cRNRJagUaIn0eRLqN8cD8iJiYzv8LMCgiLs7/k5kVz0ccZn0nepjuqU533iyY3oTHIa0COXGY9Z2TCt4fSKf/j+ROxQAfA36TTi8geRZD5zPiO58CZ1bx/GvGLJshkv5QMP/LiOg8JXewpIdIfpDNTsvOBq6R9AWSJyN+Ii3/HDBX0ukkRxZnktw51azieYzDrA+kYxyNEVGOZzqYlZS7qszMLBMfcZiZWSY+4jAzs0ycOMzMLBMnDjMzy8SJw8zMMnHiMDOzTP4/Z5r25ClCDOAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.         1.         1.        ]\n",
      " [2.         1.5        1.66666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a1=np.array([[3,2,3],[2,3,5]],dtype=int)\n",
    "a2=np.array([1,2,3],dtype=int)\n",
    "print (a1/a2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
