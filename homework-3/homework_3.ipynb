{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-3: ConvNet for MNIST Classification\n",
    "\n",
    "### **Deadline: 2018.11.18 23:59:59**\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement forward and backward for ConvLayer (`layers/conv_layer.py`)\n",
    "- #### implement forward and backward for PoolingLayer (`layers/pooling_layer.py`)\n",
    "- #### implement forward and backward for ReshapeLayer (`layers/reshape_layer.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [1, 28, 28])\n",
    "    \n",
    "    #归一化为0均值宽度为1的数据\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image) \n",
    "    \n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n",
    "You can modify hyperparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.005\n",
    "\n",
    "disp_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "sgd = SGD(learning_rate, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, ReLULayer, ConvLayer, MaxPoolingLayer, ReshapeLayer\n",
    "\n",
    "convNet = Network()\n",
    "\n",
    "# Build ConvNet with ConvLayer and PoolingLayer\n",
    "convNet.add(ConvLayer(1, 8, 3, 1))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(MaxPoolingLayer(2, 0))\n",
    "convNet.add(ConvLayer(8, 16, 3, 1))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(MaxPoolingLayer(2, 0))\n",
    "convNet.add(ReshapeLayer((batch_size, 16, 7, 7), (batch_size, 784)))\n",
    "convNet.add(FCLayer(784, 128))\n",
    "convNet.add(ReLULayer())\n",
    "convNet.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][10]\t Batch [0][550]\t Training Loss 12.9647\t Accuracy 0.0600\n",
      "Epoch [0][10]\t Batch [10][550]\t Training Loss 3.9533\t Accuracy 0.1345\n",
      "Epoch [0][10]\t Batch [20][550]\t Training Loss 3.1434\t Accuracy 0.1452\n",
      "Epoch [0][10]\t Batch [30][550]\t Training Loss 2.8235\t Accuracy 0.1813\n",
      "Epoch [0][10]\t Batch [40][550]\t Training Loss 2.6435\t Accuracy 0.2027\n",
      "Epoch [0][10]\t Batch [50][550]\t Training Loss 2.5119\t Accuracy 0.2322\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "convNet, conv_loss, conv_acc = train(convNet, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "test(convNet, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_and_acc({'ConvNet': [conv_loss, conv_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have finished homework-3, congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy as np\n",
    "batch_size=3\n",
    "pic=np.array([[1,2,3,4],[5,6,7,8],[9,20,11,12],[13,14,19,16]])\n",
    "print(pic)\n",
    "output=np.zeros((2,2))\n",
    "flag=np.array([[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]])\n",
    "kernel_size=2\n",
    "row=0\n",
    "col=0\n",
    "for m in range(2):\n",
    "    col=0\n",
    "    for n in range(2):        \n",
    "        output[m][n]=np.max(pic[row:row+kernel_size,col:col+kernel_size])\n",
    "        x=np.argmax(pic[row:row+kernel_size,col:col+kernel_size])//kernel_size\n",
    "        y=np.argmax(pic[row:row+kernel_size,col:col+kernel_size])%kernel_size\n",
    "        x+=kernel_size*m\n",
    "        y+=kernel_size*n\n",
    "        flag[x][y]=1\n",
    "        col+=kernel_size\n",
    "    row+=kernel_size\n",
    "print(flag)\n",
    "print(output)\n",
    "#output=np.array([1,2,3,4,5,6,7,8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
