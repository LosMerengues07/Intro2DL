{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.fields {'text': <torchtext.data.field.Field object at 0x7f532c1253c8>, 'label': <torchtext.data.field.Field object at 0x7f532c125470>}\n",
      "len(train) 8544\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "from torch import nn,functional\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe, CharNGram, FastText\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################\n",
    "# DataLoader\n",
    "################################\n",
    "\n",
    "# set up fields\n",
    "TEXT = data.Field()\n",
    "LABEL = data.Field(sequential=False,dtype=torch.long)\n",
    "\n",
    "# make splits for data\n",
    "# DO NOT MODIFY: fine_grained=True, train_subtrees=False\n",
    "train, val, test = datasets.SST.splits(\n",
    "    TEXT, LABEL, fine_grained=True, train_subtrees=False)\n",
    "\n",
    "# print information about the data\n",
    "print('train.fields', train.fields)\n",
    "print('len(train)', len(train))\n",
    "print('vars(train[0])', vars(train[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', ',', 'the', 'and', 'a', 'of', 'to', \"'s\"] \n",
      "\n",
      "defaultdict(<function _default_unk_index at 0x7f52ddcd6f28>, {'<unk>': 0, 'positive': 1, 'negative': 2, 'neutral': 3, 'very positive': 4, 'very negative': 5}) \n",
      "\n",
      "[('.', 8024), (',', 7131), ('the', 6037), ('and', 4431), ('a', 4403), ('of', 4386), ('to', 2995), (\"'s\", 2544), ('is', 2536), ('that', 1915), ('in', 1789), ('it', 1775), ('The', 1265), ('as', 1200), ('film', 1152), ('but', 1076), ('with', 1071), ('for', 963), ('movie', 959), ('its', 912)] \n",
      "\n",
      "len(TEXT.vocab) 18280 \n",
      "\n",
      "TEXT.vocab.vectors.size() torch.Size([18280, 300]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary\n",
    "# you can use other pretrained vectors, refer to https://github.com/pytorch/text/blob/master/torchtext/vocab.py\n",
    "TEXT.build_vocab(train, vectors=Vectors(name='vector.txt', cache='./data'))\n",
    "LABEL.build_vocab(train)\n",
    "# We can also see the vocabulary directly using either the stoi (string to int) or itos (int to string) method.\n",
    "print(TEXT.vocab.itos[:10],\"\\n\")\n",
    "print(LABEL.vocab.stoi,\"\\n\")\n",
    "print(TEXT.vocab.freqs.most_common(20),\"\\n\")\n",
    "\n",
    "# print vocab information\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab),\"\\n\")\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size(),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18280, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self,embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, labels, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0.5)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.dropout(self.embedding(inputs))\n",
    "        states, hidden = self.encoder(embeddings)\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        \n",
    "        out = self.dropout(self.decoder(encoding))\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentNet(\n",
       "  (embedding): Embedding(18280, 300)\n",
       "  (encoder): LSTM(300, 100, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (decoder): Linear(in_features=400, out_features=5, bias=True)\n",
       "  (softmax): Softmax()\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 300\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "labels = 5\n",
    "batch_size=128\n",
    "device = torch.device('cuda:7')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "model = SentimentNet(embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional,labels=labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.1, momentum=0.9,weight_decay=1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/150..  Train Loss: 1.597..  Train_Acc: 0.262..  Val Loss: 1.587..  Val_Acc: 0.242\n",
      "Epoch: 2/150..  Train Loss: 1.591..  Train_Acc: 0.266..  Val Loss: 1.584..  Val_Acc: 0.261\n",
      "Epoch: 3/150..  Train Loss: 1.591..  Train_Acc: 0.270..  Val Loss: 1.584..  Val_Acc: 0.242\n",
      "Epoch: 4/150..  Train Loss: 1.589..  Train_Acc: 0.269..  Val Loss: 1.580..  Val_Acc: 0.303\n",
      "Epoch: 5/150..  Train Loss: 1.588..  Train_Acc: 0.278..  Val Loss: 1.578..  Val_Acc: 0.270\n",
      "Epoch: 6/150..  Train Loss: 1.586..  Train_Acc: 0.278..  Val Loss: 1.570..  Val_Acc: 0.315\n",
      "Epoch: 7/150..  Train Loss: 1.575..  Train_Acc: 0.291..  Val Loss: 1.550..  Val_Acc: 0.332\n",
      "Epoch: 8/150..  Train Loss: 1.564..  Train_Acc: 0.320..  Val Loss: 1.531..  Val_Acc: 0.356\n",
      "Epoch: 9/150..  Train Loss: 1.563..  Train_Acc: 0.320..  Val Loss: 1.529..  Val_Acc: 0.343\n",
      "Epoch: 10/150..  Train Loss: 1.558..  Train_Acc: 0.324..  Val Loss: 1.542..  Val_Acc: 0.318\n",
      "Epoch: 11/150..  Train Loss: 1.556..  Train_Acc: 0.314..  Val Loss: 1.523..  Val_Acc: 0.355\n",
      "Epoch: 12/150..  Train Loss: 1.550..  Train_Acc: 0.327..  Val Loss: 1.516..  Val_Acc: 0.343\n",
      "Epoch: 13/150..  Train Loss: 1.540..  Train_Acc: 0.332..  Val Loss: 1.513..  Val_Acc: 0.364\n",
      "Epoch: 14/150..  Train Loss: 1.548..  Train_Acc: 0.331..  Val Loss: 1.512..  Val_Acc: 0.355\n",
      "Epoch: 15/150..  Train Loss: 1.543..  Train_Acc: 0.334..  Val Loss: 1.500..  Val_Acc: 0.371\n",
      "Epoch: 16/150..  Train Loss: 1.543..  Train_Acc: 0.336..  Val Loss: 1.497..  Val_Acc: 0.370\n",
      "Epoch: 17/150..  Train Loss: 1.538..  Train_Acc: 0.338..  Val Loss: 1.493..  Val_Acc: 0.374\n",
      "Epoch: 18/150..  Train Loss: 1.532..  Train_Acc: 0.350..  Val Loss: 1.499..  Val_Acc: 0.367\n",
      "Epoch: 19/150..  Train Loss: 1.537..  Train_Acc: 0.345..  Val Loss: 1.528..  Val_Acc: 0.329\n",
      "Epoch: 20/150..  Train Loss: 1.535..  Train_Acc: 0.342..  Val Loss: 1.498..  Val_Acc: 0.363\n",
      "Epoch: 21/150..  Train Loss: 1.532..  Train_Acc: 0.345..  Val Loss: 1.494..  Val_Acc: 0.378\n",
      "Epoch: 22/150..  Train Loss: 1.532..  Train_Acc: 0.348..  Val Loss: 1.509..  Val_Acc: 0.356\n",
      "Epoch: 23/150..  Train Loss: 1.528..  Train_Acc: 0.353..  Val Loss: 1.482..  Val_Acc: 0.388\n",
      "Epoch: 24/150..  Train Loss: 1.524..  Train_Acc: 0.356..  Val Loss: 1.509..  Val_Acc: 0.353\n",
      "Epoch: 25/150..  Train Loss: 1.529..  Train_Acc: 0.354..  Val Loss: 1.493..  Val_Acc: 0.374\n",
      "Epoch: 26/150..  Train Loss: 1.525..  Train_Acc: 0.360..  Val Loss: 1.489..  Val_Acc: 0.378\n",
      "Epoch: 27/150..  Train Loss: 1.525..  Train_Acc: 0.358..  Val Loss: 1.488..  Val_Acc: 0.378\n",
      "Epoch: 28/150..  Train Loss: 1.523..  Train_Acc: 0.360..  Val Loss: 1.488..  Val_Acc: 0.382\n",
      "Epoch: 29/150..  Train Loss: 1.520..  Train_Acc: 0.364..  Val Loss: 1.473..  Val_Acc: 0.396\n",
      "Epoch: 30/150..  Train Loss: 1.519..  Train_Acc: 0.366..  Val Loss: 1.469..  Val_Acc: 0.405\n",
      "Epoch: 31/150..  Train Loss: 1.525..  Train_Acc: 0.356..  Val Loss: 1.487..  Val_Acc: 0.382\n",
      "Epoch: 32/150..  Train Loss: 1.520..  Train_Acc: 0.365..  Val Loss: 1.481..  Val_Acc: 0.390\n",
      "Epoch: 33/150..  Train Loss: 1.513..  Train_Acc: 0.375..  Val Loss: 1.505..  Val_Acc: 0.368\n",
      "Epoch: 34/150..  Train Loss: 1.513..  Train_Acc: 0.377..  Val Loss: 1.473..  Val_Acc: 0.401\n",
      "Epoch: 35/150..  Train Loss: 1.514..  Train_Acc: 0.373..  Val Loss: 1.492..  Val_Acc: 0.383\n",
      "Epoch: 36/150..  Train Loss: 1.524..  Train_Acc: 0.360..  Val Loss: 1.503..  Val_Acc: 0.359\n",
      "Epoch: 37/150..  Train Loss: 1.514..  Train_Acc: 0.368..  Val Loss: 1.469..  Val_Acc: 0.410\n",
      "Epoch: 38/150..  Train Loss: 1.508..  Train_Acc: 0.375..  Val Loss: 1.469..  Val_Acc: 0.402\n",
      "Epoch: 39/150..  Train Loss: 1.510..  Train_Acc: 0.377..  Val Loss: 1.465..  Val_Acc: 0.410\n",
      "Epoch: 40/150..  Train Loss: 1.507..  Train_Acc: 0.376..  Val Loss: 1.462..  Val_Acc: 0.417\n",
      "Epoch: 41/150..  Train Loss: 1.508..  Train_Acc: 0.384..  Val Loss: 1.469..  Val_Acc: 0.409\n",
      "Epoch: 42/150..  Train Loss: 1.511..  Train_Acc: 0.375..  Val Loss: 1.508..  Val_Acc: 0.371\n",
      "Epoch: 43/150..  Train Loss: 1.517..  Train_Acc: 0.367..  Val Loss: 1.465..  Val_Acc: 0.415\n",
      "Epoch: 44/150..  Train Loss: 1.501..  Train_Acc: 0.387..  Val Loss: 1.512..  Val_Acc: 0.367\n",
      "Epoch: 45/150..  Train Loss: 1.507..  Train_Acc: 0.379..  Val Loss: 1.481..  Val_Acc: 0.405\n",
      "Epoch: 46/150..  Train Loss: 1.514..  Train_Acc: 0.372..  Val Loss: 1.467..  Val_Acc: 0.411\n",
      "Epoch: 47/150..  Train Loss: 1.501..  Train_Acc: 0.381..  Val Loss: 1.472..  Val_Acc: 0.405\n",
      "Epoch: 48/150..  Train Loss: 1.511..  Train_Acc: 0.378..  Val Loss: 1.503..  Val_Acc: 0.370\n",
      "Epoch: 49/150..  Train Loss: 1.501..  Train_Acc: 0.385..  Val Loss: 1.456..  Val_Acc: 0.418\n",
      "Epoch: 50/150..  Train Loss: 1.512..  Train_Acc: 0.370..  Val Loss: 1.457..  Val_Acc: 0.417\n",
      "Epoch: 51/150..  Train Loss: 1.503..  Train_Acc: 0.380..  Val Loss: 1.467..  Val_Acc: 0.410\n",
      "Epoch: 52/150..  Train Loss: 1.503..  Train_Acc: 0.380..  Val Loss: 1.455..  Val_Acc: 0.416\n",
      "Epoch: 53/150..  Train Loss: 1.501..  Train_Acc: 0.385..  Val Loss: 1.456..  Val_Acc: 0.407\n",
      "Epoch: 54/150..  Train Loss: 1.499..  Train_Acc: 0.385..  Val Loss: 1.454..  Val_Acc: 0.421\n",
      "Epoch: 55/150..  Train Loss: 1.503..  Train_Acc: 0.381..  Val Loss: 1.461..  Val_Acc: 0.415\n",
      "Epoch: 56/150..  Train Loss: 1.498..  Train_Acc: 0.393..  Val Loss: 1.494..  Val_Acc: 0.381\n",
      "Epoch: 57/150..  Train Loss: 1.503..  Train_Acc: 0.385..  Val Loss: 1.472..  Val_Acc: 0.412\n",
      "Epoch: 58/150..  Train Loss: 1.497..  Train_Acc: 0.391..  Val Loss: 1.449..  Val_Acc: 0.428\n",
      "Epoch: 59/150..  Train Loss: 1.495..  Train_Acc: 0.391..  Val Loss: 1.486..  Val_Acc: 0.386\n",
      "Epoch: 60/150..  Train Loss: 1.496..  Train_Acc: 0.391..  Val Loss: 1.459..  Val_Acc: 0.406\n",
      "Epoch: 61/150..  Train Loss: 1.497..  Train_Acc: 0.388..  Val Loss: 1.451..  Val_Acc: 0.427\n",
      "Epoch: 62/150..  Train Loss: 1.499..  Train_Acc: 0.387..  Val Loss: 1.460..  Val_Acc: 0.414\n",
      "Epoch: 63/150..  Train Loss: 1.494..  Train_Acc: 0.389..  Val Loss: 1.455..  Val_Acc: 0.429\n",
      "Epoch: 64/150..  Train Loss: 1.492..  Train_Acc: 0.395..  Val Loss: 1.457..  Val_Acc: 0.410\n",
      "Epoch: 65/150..  Train Loss: 1.493..  Train_Acc: 0.395..  Val Loss: 1.451..  Val_Acc: 0.415\n",
      "Epoch: 66/150..  Train Loss: 1.489..  Train_Acc: 0.396..  Val Loss: 1.478..  Val_Acc: 0.408\n",
      "Epoch: 67/150..  Train Loss: 1.486..  Train_Acc: 0.394..  Val Loss: 1.445..  Val_Acc: 0.429\n",
      "Epoch: 68/150..  Train Loss: 1.486..  Train_Acc: 0.403..  Val Loss: 1.463..  Val_Acc: 0.408\n",
      "Epoch: 69/150..  Train Loss: 1.496..  Train_Acc: 0.392..  Val Loss: 1.451..  Val_Acc: 0.432\n",
      "Epoch: 70/150..  Train Loss: 1.483..  Train_Acc: 0.403..  Val Loss: 1.453..  Val_Acc: 0.424\n",
      "Epoch: 71/150..  Train Loss: 1.492..  Train_Acc: 0.396..  Val Loss: 1.459..  Val_Acc: 0.413\n",
      "Epoch: 72/150..  Train Loss: 1.487..  Train_Acc: 0.398..  Val Loss: 1.459..  Val_Acc: 0.410\n",
      "Epoch: 73/150..  Train Loss: 1.493..  Train_Acc: 0.395..  Val Loss: 1.479..  Val_Acc: 0.396\n",
      "Epoch: 74/150..  Train Loss: 1.485..  Train_Acc: 0.403..  Val Loss: 1.454..  Val_Acc: 0.424\n",
      "Epoch: 75/150..  Train Loss: 1.491..  Train_Acc: 0.394..  Val Loss: 1.450..  Val_Acc: 0.431\n",
      "Epoch: 76/150..  Train Loss: 1.491..  Train_Acc: 0.390..  Val Loss: 1.460..  Val_Acc: 0.420\n",
      "Epoch: 77/150..  Train Loss: 1.492..  Train_Acc: 0.394..  Val Loss: 1.491..  Val_Acc: 0.394\n",
      "Epoch: 78/150..  Train Loss: 1.494..  Train_Acc: 0.388..  Val Loss: 1.448..  Val_Acc: 0.424\n",
      "Epoch: 79/150..  Train Loss: 1.492..  Train_Acc: 0.391..  Val Loss: 1.452..  Val_Acc: 0.432\n",
      "Epoch: 80/150..  Train Loss: 1.480..  Train_Acc: 0.402..  Val Loss: 1.458..  Val_Acc: 0.418\n",
      "Epoch: 81/150..  Train Loss: 1.493..  Train_Acc: 0.391..  Val Loss: 1.453..  Val_Acc: 0.423\n",
      "Epoch: 82/150..  Train Loss: 1.485..  Train_Acc: 0.401..  Val Loss: 1.454..  Val_Acc: 0.424\n",
      "Epoch: 83/150..  Train Loss: 1.483..  Train_Acc: 0.405..  Val Loss: 1.445..  Val_Acc: 0.431\n",
      "Epoch: 84/150..  Train Loss: 1.484..  Train_Acc: 0.401..  Val Loss: 1.462..  Val_Acc: 0.413\n",
      "Epoch: 85/150..  Train Loss: 1.488..  Train_Acc: 0.394..  Val Loss: 1.452..  Val_Acc: 0.421\n",
      "Epoch: 86/150..  Train Loss: 1.478..  Train_Acc: 0.410..  Val Loss: 1.452..  Val_Acc: 0.422\n",
      "Epoch: 87/150..  Train Loss: 1.479..  Train_Acc: 0.407..  Val Loss: 1.445..  Val_Acc: 0.432\n",
      "Epoch: 88/150..  Train Loss: 1.478..  Train_Acc: 0.410..  Val Loss: 1.446..  Val_Acc: 0.426\n",
      "Epoch: 89/150..  Train Loss: 1.478..  Train_Acc: 0.413..  Val Loss: 1.445..  Val_Acc: 0.432\n",
      "Epoch: 90/150..  Train Loss: 1.478..  Train_Acc: 0.403..  Val Loss: 1.452..  Val_Acc: 0.414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91/150..  Train Loss: 1.474..  Train_Acc: 0.411..  Val Loss: 1.453..  Val_Acc: 0.425\n",
      "Epoch: 92/150..  Train Loss: 1.482..  Train_Acc: 0.404..  Val Loss: 1.467..  Val_Acc: 0.411\n",
      "Epoch: 93/150..  Train Loss: 1.479..  Train_Acc: 0.401..  Val Loss: 1.445..  Val_Acc: 0.423\n",
      "Epoch: 94/150..  Train Loss: 1.477..  Train_Acc: 0.406..  Val Loss: 1.438..  Val_Acc: 0.439\n",
      "Epoch: 95/150..  Train Loss: 1.483..  Train_Acc: 0.406..  Val Loss: 1.446..  Val_Acc: 0.437\n",
      "Epoch: 96/150..  Train Loss: 1.483..  Train_Acc: 0.404..  Val Loss: 1.444..  Val_Acc: 0.432\n",
      "Epoch: 97/150..  Train Loss: 1.483..  Train_Acc: 0.397..  Val Loss: 1.461..  Val_Acc: 0.413\n",
      "Epoch: 98/150..  Train Loss: 1.480..  Train_Acc: 0.405..  Val Loss: 1.448..  Val_Acc: 0.422\n",
      "Epoch: 99/150..  Train Loss: 1.471..  Train_Acc: 0.415..  Val Loss: 1.447..  Val_Acc: 0.431\n",
      "Epoch: 100/150..  Train Loss: 1.471..  Train_Acc: 0.415..  Val Loss: 1.448..  Val_Acc: 0.426\n",
      "Epoch: 101/150..  Train Loss: 1.479..  Train_Acc: 0.406..  Val Loss: 1.465..  Val_Acc: 0.416\n",
      "Epoch: 102/150..  Train Loss: 1.480..  Train_Acc: 0.404..  Val Loss: 1.441..  Val_Acc: 0.431\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train, val, test), batch_size=batch_size,shuffle=True)\n",
    "epochs = 150\n",
    "train_losses, validation_losses,validation_accs = [] ,[],[]\n",
    "for epoch in range(epochs):\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    running_loss=0\n",
    "    acc=0\n",
    "    for batch in train_iter:\n",
    "        text=batch.text.to(device)\n",
    "        label=batch.label-1\n",
    "        #label=label>=3\n",
    "        #label=label.long()\n",
    "        label=label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(text) \n",
    "        loss = criterion(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        acc+=torch.sum(torch.argmax(output,1)==label).cpu().item()/128.0\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "    val_loss=0\n",
    "    val_acc=0\n",
    "    for val_batch in val_iter:\n",
    "        val_text=val_batch.text.to(device)\n",
    "        val_label=val_batch.label-1\n",
    "        #val_label=val_label>=3\n",
    "        #val_label=val_label.long()\n",
    "        val_label=val_label.to(device)\n",
    "        val_output = model.forward(val_text) \n",
    "        \n",
    "        val_loss += criterion(val_output,val_label).item()\n",
    "        val_acc += torch.sum(torch.argmax(val_output,1)==val_label).cpu().item()/128.0\n",
    "\n",
    "    \n",
    "    train_losses.append(running_loss/len(train_iter))\n",
    "    validation_losses.append(val_loss/len(val_iter))\n",
    "    validation_accs.append(val_acc/len(val_iter))\n",
    "        \n",
    "    print(\"Epoch: {}/{}.. \".format(epoch+1, epochs),\n",
    "              \"Train Loss: {:.3f}.. \".format(running_loss/len(train_iter)),\n",
    "              \"Train_Acc: {:.3f}.. \".format(acc/len(train_iter)),\n",
    "              \"Val Loss: {:.3f}.. \".format(val_loss/len(val_iter)),\n",
    "              \"Val_Acc: {:.3f}\".format(val_acc/len(val_iter)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot image\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(validation_losses, label='Validation loss')\n",
    "plt.plot(validation_accs,label='Validation Accuracy')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make iterator for splits\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "test_loss=0\n",
    "test_acc=0\n",
    "for batch in test_iter:\n",
    "    text=batch.text.to(device)\n",
    "    label=batch.label-1\n",
    "    #label=label>=3\n",
    "    #label=label.long()\n",
    "    label=label.to(device)\n",
    "    \n",
    "    output = model.forward(text) \n",
    "    test_loss += criterion(output,label).item()\n",
    "\n",
    "    #print(torch.sum(torch.argmax(output,1)==label).cpu().item()/len(label))\n",
    "    test_acc += torch.sum(torch.argmax(output,1)==label).cpu().item()/128.0\n",
    "\n",
    "print(\"Test Loss: {:.3f}.. \".format(test_loss/len(test_iter)),\n",
    "        \"Test Accuracy: {:.3f}\".format(test_acc/len(test_iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
